{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd803e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c4ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "from pyDOE import lhs\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "plt.rcParams.update({'font.size':18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d2dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1024):\n",
    "#     random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "#     torch.use_deterministic_algorithms(True)  # 有检查操作，看下文区别\n",
    " \n",
    "seed_torch(314)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ecd45",
   "metadata": {},
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce93def",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.pi\n",
    "pf=1000;# kg/m-3\n",
    "ps=2700;\n",
    "pp=5540;\n",
    "p=1000;# 水的密度\n",
    "d31=-210e-12;\n",
    "Cp=89.5e-9;\n",
    "hp=0.3e-3;\n",
    "hs=0.15e-3;\n",
    "ha=-(hs+hp)/2;\n",
    "hb=(hs-hp)/2;\n",
    "hc=(hs+hp)/2;\n",
    "Es=70e9; # KN/m2\n",
    "Ep=15.857e9;# KN/m2\n",
    "D=16e-3;\n",
    "Lc=45e-3;\n",
    "M0=0.01;\n",
    "b=18e-3;\n",
    "L=100e-3;\n",
    "Mf=pf*pi*D**2*Lc/4+(33/140)*pi*pf*b**2*(L/4);\n",
    "M=M0+Mf;\n",
    "It=(M*(D/2)**2)/2;\n",
    "m=ps*hs*b+pp*hp*b;\n",
    "k1=0.086;# 阻尼比\n",
    "K=b*(Es*(hb**3-ha**3)+Ep*(hc**3-hb**3))/3;\n",
    "CD=2;\n",
    "C1=0.3;\n",
    "U=1;  # 速度\n",
    "lam=1;\n",
    "keci=0.3;\n",
    "A=12;\n",
    "St=0.2;\n",
    "wf=2*pi*St*U/D;\n",
    "R=0.5e6;\n",
    "\n",
    "e31=-Ep*d31;\n",
    "theta=-e31*b*(hc**2-hb**2)/(2*hp);\n",
    "lamda=0.808646;\n",
    "wn=lamda**2*(K/(m*L**4))**0.5; # 固有频率\n",
    "\n",
    "#f_L=double(subs(f,x,L));\n",
    "#df_L=double(subs(df,x,L));\n",
    "#fi=(f_L+0.5*D*df_L);\n",
    "f_L=-6.1894;\n",
    "df_L=-92.8022;\n",
    "fi=-6.9318;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211dfbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = (0, 1.5, -0.0025, 0.0025)\n",
    "tmin, tmax, xmin, xmax = domain\n",
    "backbone_layers = [2] + [20]*4 + [1]\n",
    "# nn_lam_layers = [1] + [20]*3 + [2]\n",
    "adam_iters = 40000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = r'./model'\n",
    "train_info_path = r'./'\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189dba40",
   "metadata": {},
   "source": [
    "## 数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63f7fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2, 5) (100, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, domain):\n",
    "        self.domain = domain\n",
    "    \n",
    "    def train_data(self, verbose=None):\n",
    "        tmin, tmax, xmin, xmax = self.domain\n",
    "        # 内部点采样\n",
    "        t_res = np.linspace(tmin, tmax, 50)\n",
    "        x_res = np.linspace(xmin, xmax, 80)\n",
    "        X_res = self.sample_xy(t_res, x_res)\n",
    "        \n",
    "        X_res = np.expand_dims(X_res, axis=2)\n",
    "        X_res = np.repeat(X_res,5,axis=2)\n",
    "        \n",
    "        # 初始点采样\n",
    "        X_ics =self.sample_xy(np.array([tmin]), np.linspace(xmin, xmax, 100))\n",
    "#         X_ics = np.concatenate([X_ics, self.sample_xy(np.array([tmin]), np.linspace(xmin,xmax,100))], axis=0)\n",
    "        X_ics = np.expand_dims(X_ics, axis=2)\n",
    "        X_ics = np.repeat(X_ics,5,axis=2)\n",
    "        return X_res, X_ics\n",
    "    \n",
    "    def sample_xy(self, x, y):\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        X = np.concatenate([xx.reshape((-1, 1)), yy.reshape((-1, 1))], axis=1)\n",
    "        return X\n",
    "\n",
    "    \n",
    "dataset = Dataset(domain)\n",
    "X_res, X_ics = dataset.train_data()\n",
    "# X_res_1 = X_res_2 =X_res_3 =X_res_4 =X_res_5 =X_res\n",
    "print(X_res.shape, X_ics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02b722",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "852c7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(mlp_layers)-2):\n",
    "            layer = nn.Sequential()\n",
    "            layer.add_module(f'fc{i}', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            layer.add_module(f'act{i}', nn.Tanh())\n",
    "            self.model.add_module(f'layer{i}', layer)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'layer{len(mlp_layers)-2}', last_layer)\n",
    "        \n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) > 1:\n",
    "#                 nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "backbone1 = MLP(backbone_layers)\n",
    "backbone2 = MLP(backbone_layers)\n",
    "backbone3 = MLP(backbone_layers)\n",
    "backbone4 = MLP(backbone_layers)\n",
    "backbone5 = MLP(backbone_layers)\n",
    "\n",
    "# nn_lam = MLP(nn_lam_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5963b8",
   "metadata": {},
   "source": [
    "## 主干网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf72473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eebf1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, backbone1, backbone2, backbone3, backbone4, backbone5, mu=None, sigma=None):\n",
    "        super(PINN, self).__init__()\n",
    "        self.backbone1 = backbone1\n",
    "        self.backbone2 = backbone2\n",
    "        self.backbone3 = backbone3\n",
    "        self.backbone4 = backbone4\n",
    "        self.backbone5 = backbone5\n",
    "\n",
    "    def forward(self, X_res, X_ics):\n",
    "        \n",
    "        loss_res_1 = torch.mean(self.net_E1(X_res) ** 2)\n",
    "        loss_res_2 = torch.mean(self.net_E2(X_res) ** 2)\n",
    "        loss_res_3 = torch.mean(self.net_E3(X_res) ** 2)\n",
    "        loss_res_4 = torch.mean(self.net_E4(X_res) ** 2)\n",
    "        loss_res_5 = torch.mean(self.net_E5(X_res) ** 2)\n",
    "        loss_ics = torch.mean(self.net_ics(X_ics)**2)\n",
    "        return loss_res_1, loss_res_2, loss_res_3, loss_res_4, loss_res_5, loss_ics \n",
    "\n",
    "    def net_pred1(self, X):\n",
    "        return self.backbone1(X)\n",
    "    \n",
    "    def net_pred2(self, X):\n",
    "        return self.backbone2(X)\n",
    "    \n",
    "    def net_pred3(self, X):\n",
    "        return self.backbone3(X)\n",
    "    \n",
    "    def net_pred4(self, X):\n",
    "        return self.backbone4(X)\n",
    "    \n",
    "    def net_pred5(self, X):\n",
    "        return self.backbone5(X)\n",
    "    \n",
    "    def net_X_res_pred(self, X):\n",
    "        X1_star = X[:,:,0]\n",
    "        X2_star = X[:,:,1]\n",
    "        X3_star = X[:,:,2]\n",
    "        X4_star = X[:,:,3]\n",
    "        X5_star = X[:,:,4]\n",
    "        \n",
    "        X1_star.requires_grad_(True)\n",
    "        X2_star.requires_grad_(True)\n",
    "        X3_star.requires_grad_(True)\n",
    "        X4_star.requires_grad_(True)\n",
    "        X5_star.requires_grad_(True)\n",
    "        \n",
    "        X1 = self.net_pred1(X1_star)\n",
    "        X2 = self.net_pred2(X2_star)\n",
    "        X3 = self.net_pred3(X3_star)\n",
    "        X4 = self.net_pred4(X4_star)\n",
    "        X5 = self.net_pred5(X5_star)\n",
    "        return X1,X2,X3,X4,X5\n",
    "        \n",
    "\n",
    "    def net_E1(self, X):\n",
    "        X.requires_grad_(True)\n",
    "        \n",
    "        X1_star = X[:,:,0]\n",
    "        X2_star = X[:,:,1]\n",
    "        X1_star.requires_grad_(True)\n",
    "        X2_star.requires_grad_(True)\n",
    "        \n",
    "        X1 = self.net_pred1(X1_star)\n",
    "        X2 = self.net_pred2(X2_star)\n",
    "\n",
    "        grad_X1 = grad(X1, X1_star)[0]\n",
    "        X1_pred_t = grad_X1[:, [0]]\n",
    "        \n",
    "        return X1_pred_t - X2  \n",
    "    \n",
    "    def net_E2(self,X):\n",
    "        X1_star = X[:,:,0]\n",
    "        X2_star = X[:,:,1]\n",
    "        X3_star = X[:,:,2]\n",
    "        X4_star = X[:,:,3]\n",
    "        X5_star = X[:,:,4]\n",
    "        \n",
    "        X1_star.requires_grad_(True)\n",
    "        X2_star.requires_grad_(True)\n",
    "        X3_star.requires_grad_(True)\n",
    "        X5_star.requires_grad_(True)\n",
    "\n",
    "        X1 = self.net_pred1(X1_star)\n",
    "        X2 = self.net_pred2(X2_star)\n",
    "        X3 = self.net_pred3(X3_star)\n",
    "        X5 = self.net_pred5(X5_star)\n",
    "        \n",
    "        grad_X = grad(X2, X2_star)[0]\n",
    "        X2_pred_t = grad_X[:, [0]]\n",
    "        \n",
    "        return X2_pred_t + (2*k1*wn+0.5*CD*p*D*U*Lc*fi**2)*X2-wn**2*X1-theta*df_L*X5+0.25*C1*p*D*U**2*Lc*fi*X3\n",
    "    \n",
    "    def net_E3(self, X):\n",
    "        X1_star = X[:,:,0]\n",
    "        X2_star = X[:,:,1]\n",
    "        X3_star = X[:,:,2]\n",
    "        X4_star = X[:,:,3]\n",
    "        X5_star = X[:,:,4]\n",
    "        \n",
    "        X1_star.requires_grad_(True)\n",
    "        X3_star.requires_grad_(True)\n",
    "        \n",
    "        X3 = self.net_pred3(X3_star)\n",
    "        X4 = self.net_pred4(X4_star)\n",
    "        \n",
    "        grad_X = grad(X3, X3_star)[0]\n",
    "        X3_pred_t = grad_X[:, [0]]\n",
    "        return X3_pred_t-X4\n",
    "    \n",
    "    def net_E4(self, X):\n",
    "        X1_star = X[:,:,0]\n",
    "        X2_star = X[:,:,1]\n",
    "        X3_star = X[:,:,2]\n",
    "        X4_star = X[:,:,3]\n",
    "        X5_star = X[:,:,4]\n",
    "        \n",
    "        X4_star.requires_grad_(True)\n",
    "        X1 = self.net_pred1(X1_star)\n",
    "        X2 = self.net_pred2(X2_star)\n",
    "        X3 = self.net_pred3(X3_star)\n",
    "        X4 = self.net_pred4(X4_star)\n",
    "        X5 = self.net_pred5(X5_star)\n",
    "        \n",
    "        grad_X = grad(X4, X4_star)[0]\n",
    "        X4_pred_t = grad_X[:, [0]]\n",
    "        \n",
    "        return X4_pred_t - (-lam*keci*wf*(X3)**2*X4+lam*keci*wf*X4-wf**2*X3+A/D*fi*(-(2*k1*wn+0.5*CD*p*D*U*Lc*fi**2)*X2-wn**2*X1-theta*df_L*X5+0.25*C1*p*D*U**2*Lc*fi*X3))\n",
    "    \n",
    "    def net_E5(self,X):\n",
    "        X1_star = X[:,:,0]\n",
    "        X2_star = X[:,:,1]\n",
    "        X3_star = X[:,:,2]\n",
    "        X4_star = X[:,:,3]\n",
    "        X5_star = X[:,:,4]\n",
    "        \n",
    "        X4_star.requires_grad_(True)\n",
    "        X2 = self.net_pred2(X2_star)\n",
    "        X5 = self.net_pred5(X5_star)\n",
    "        \n",
    "        grad_X = grad(X5, X5_star)[0]\n",
    "        X5_pred_t = grad_X[:, [0]]\n",
    "        \n",
    "        return X5_pred_t-((-X5/R+theta*df_L*X2)/Cp)\n",
    "        \n",
    "    def net_ics(self,X):\n",
    "        X1 = X[:,1,0]\n",
    "        X2 = X[:,1,1:]\n",
    "        X1 = np.add(X1, -np.ones((X1.shape))*0.0002).reshape((-1,1))\n",
    "        X_ics = torch.cat((X1,X2),1)\n",
    "        return X_ics\n",
    "        \n",
    "pinn = PINN(backbone_layers,backbone_layers,backbone_layers,backbone_layers,backbone_layers)\n",
    "# pinn.net_E1(torch.tensor(X_res_1),torch.tensor(X_res_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948e16f",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b7c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #    500/40000\tloss:1.82e+06, loss_r_1:4.74e-03, loss_r_2:2.93e+01, loss_r_3:1.21e-03,loss_r_4:3.50e+03,loss_r_5:1.47e+04 loss_i:2.13e-06  Valid # loss:6.99e+03, loss_r:4.74e-03, loss_i:2.13e-06\n",
      "Iter #   1000/40000\tloss:4.37e+05, loss_r_1:4.46e-03, loss_r_2:2.76e+01, loss_r_3:1.19e-03,loss_r_4:6.98e+02,loss_r_5:3.65e+03 loss_i:2.13e-06  Valid # loss:1.42e+03, loss_r:4.46e-03, loss_i:2.13e-06\n",
      "Iter #   1500/40000\tloss:3.74e+05, loss_r_1:4.41e-03, loss_r_2:2.73e+01, loss_r_3:1.18e-03,loss_r_4:5.75e+02,loss_r_5:3.14e+03 loss_i:2.13e-06  Valid # loss:1.18e+03, loss_r:4.41e-03, loss_i:2.13e-06\n",
      "Iter #   2000/40000\tloss:3.37e+05, loss_r_1:4.42e-03, loss_r_2:2.73e+01, loss_r_3:1.16e-03,loss_r_4:5.06e+02,loss_r_5:2.84e+03 loss_i:2.13e-06  Valid # loss:1.04e+03, loss_r:4.42e-03, loss_i:2.13e-06\n",
      "Iter #   2500/40000\tloss:2.96e+05, loss_r_1:4.35e-03, loss_r_2:2.67e+01, loss_r_3:1.09e-03,loss_r_4:5.65e+02,loss_r_5:2.37e+03 loss_i:2.13e-06  Valid # loss:1.37e+03, loss_r:4.35e-03, loss_i:2.13e-06\n",
      "Iter #   3000/40000\tloss:2.61e+05, loss_r_1:4.24e-03, loss_r_2:2.60e+01, loss_r_3:9.69e-04,loss_r_4:6.53e+02,loss_r_5:1.93e+03 loss_i:2.13e-06  Valid # loss:1.33e+03, loss_r:4.24e-03, loss_i:2.13e-06\n",
      "Iter #   3500/40000\tloss:1.59e+05, loss_r_1:3.97e-03, loss_r_2:2.44e+01, loss_r_3:7.75e-04,loss_r_4:9.50e+02,loss_r_5:6.16e+02 loss_i:2.13e-06  Valid # loss:1.92e+03, loss_r:3.97e-03, loss_i:2.13e-06\n",
      "Iter #   4000/40000\tloss:1.09e+05, loss_r_1:3.88e-03, loss_r_2:2.38e+01, loss_r_3:7.63e-04,loss_r_4:7.89e+02,loss_r_5:2.79e+02 loss_i:2.13e-06  Valid # loss:1.60e+03, loss_r:3.88e-03, loss_i:2.13e-06\n",
      "Iter #   4500/40000\tloss:2.12e+09, loss_r_1:3.59e-03, loss_r_2:3.10e+01, loss_r_3:6.34e-04,loss_r_4:2.12e+07,loss_r_5:2.54e+02 loss_i:2.13e-06  Valid # loss:1.17e+07, loss_r:3.83e-03, loss_i:2.13e-06\n",
      "Iter #   5000/40000\tloss:3.93e+08, loss_r_1:3.67e-03, loss_r_2:1.91e+01, loss_r_3:6.50e-04,loss_r_4:3.93e+06,loss_r_5:6.98e+01 loss_i:2.13e-06  Valid # loss:2.10e+06, loss_r:3.57e-03, loss_i:2.13e-06\n",
      "Iter #   5500/40000\tloss:1.13e+05, loss_r_1:3.46e-03, loss_r_2:2.12e+01, loss_r_3:5.27e-04,loss_r_4:1.08e+03,loss_r_5:2.90e+01 loss_i:2.13e-06  Valid # loss:2.11e+03, loss_r:3.46e-03, loss_i:2.13e-06\n",
      "Iter #   6000/40000\tloss:1.29e+05, loss_r_1:3.30e-03, loss_r_2:2.03e+01, loss_r_3:4.40e-04,loss_r_4:1.25e+03,loss_r_5:1.48e+01 loss_i:2.13e-06  Valid # loss:2.36e+03, loss_r:3.31e-03, loss_i:2.13e-06\n",
      "Iter #   6500/40000\tloss:5.08e+08, loss_r_1:3.23e-03, loss_r_2:1.62e+01, loss_r_3:3.97e-04,loss_r_4:5.08e+06,loss_r_5:1.62e+01 loss_i:2.13e-06  Valid # loss:5.11e+06, loss_r:3.11e-03, loss_i:2.13e-06\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(domain)\n",
    "X_res, X_ics = dataset.train_data()\n",
    "X_res = torch.from_numpy(X_res).float().to(device)\n",
    "\n",
    "X_ics = torch.from_numpy(X_ics).float().to(device)\n",
    "\n",
    "mu = X_res.mean(dim=0)\n",
    "sigma = X_res.std(dim=0)  # 求样本标准差\n",
    "\n",
    "backbone1 = MLP(backbone_layers)  # 主干网络\n",
    "backbone2 = MLP(backbone_layers)\n",
    "backbone3 = MLP(backbone_layers)\n",
    "backbone4 = MLP(backbone_layers)\n",
    "backbone5 = MLP(backbone_layers)\n",
    "pinn = PINN(backbone1, backbone2, backbone3, backbone4, backbone5, mu, sigma).to(device)\n",
    "\n",
    "optimizer_adam = optim.Adam(pinn.parameters(), lr=1e-3)\n",
    "\n",
    "lr_sche = optim.lr_scheduler.ExponentialLR(optimizer_adam, gamma=0.8)  # 指数衰减学习率\n",
    "logger = {\n",
    "    \"loss\": [], \n",
    "    \"loss_res_1\": [],\n",
    "    \"loss_res_2\": [],\n",
    "    \"loss_res_3\": [],\n",
    "    \"loss_res_4\": [],\n",
    "    \"loss_res_5\": [],\n",
    "    \"loss_ics\": [],\n",
    "    \"iter\": [],\n",
    "    \"mu\": mu,\n",
    "    \"sigma\": sigma\n",
    "}\n",
    "best_loss = 1e9\n",
    "\n",
    "# 训练\n",
    "start_time = time.time()\n",
    "for it in range(adam_iters):\n",
    "    # 计算loss并更新网络 -------\n",
    "    pinn.train()\n",
    "    pinn.zero_grad()\n",
    "    \n",
    "    loss_res_1, loss_res_2, loss_res_3, loss_res_4, loss_res_5, loss_ics   = pinn(X_res, X_ics)\n",
    "    loss = loss_res_1 + 100*loss_res_2 + loss_res_3 + 100*loss_res_4 + 100*loss_res_5 + loss_ics \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "#     optimizer_adam_lam.step()\n",
    "    # 计算loss并更新网络 -------\n",
    "    \n",
    "    \n",
    "    if (it + 1) % 100 == 0:\n",
    "        # 保存loss信息\n",
    "        pinn.train(False)\n",
    "        loss_res_1_valid, loss_res_2_valid, loss_res_3_valid, loss_res_4_valid, loss_res_5_valid, loss_ics_valid  = pinn(X_res, X_ics)\n",
    "        loss_valid = loss_res_1_valid + loss_res_2_valid + loss_res_3_valid + loss_res_4_valid + loss_res_4_valid + loss_ics_valid\n",
    "        \n",
    "        logger[\"loss\"].append(loss_valid.item())\n",
    "        logger[\"loss_res_1\"].append(loss_res_1_valid.item())\n",
    "        logger[\"loss_res_2\"].append(loss_res_2_valid.item())\n",
    "        logger[\"loss_res_3\"].append(loss_res_3_valid.item())\n",
    "        logger[\"loss_res_4\"].append(loss_res_4_valid.item())\n",
    "        logger[\"loss_res_5\"].append(loss_res_5_valid.item())\n",
    "        logger[\"loss_ics\"].append(loss_ics_valid.item())\n",
    "        logger[\"iter\"].append(it+1)\n",
    "        \n",
    "        # 保存训练loss最低的模型\n",
    "        if loss_valid.item() < best_loss:\n",
    "            model_state = {'iter': it+1, 'backbone_state': pinn.state_dict()}\n",
    "            torch.save(model_state, os.path.join(model_path, 'pinn_adam.pth'))\n",
    "            best_loss = loss_valid.item()\n",
    "        \n",
    "        if (it + 1) % 500 == 0:\n",
    "            # 保存并打印训练日志\n",
    "            info = f'Iter # {it+1:6d}/{adam_iters}\\t' + \\\n",
    "                f'loss:{loss.item():.2e}, loss_r_1:{loss_res_1.item():.2e}, loss_r_2:{loss_res_2.item():.2e}, loss_r_3:{loss_res_3.item():.2e},loss_r_4:{loss_res_4.item():.2e},loss_r_5:{loss_res_5.item():.2e} loss_i:{loss_ics.item():.2e}  ' + \\\n",
    "                f'Valid # loss:{loss_valid.item():.2e}, loss_r:{loss_res_1_valid.item():.2e}, loss_i:{loss_ics_valid.item():.2e}'\n",
    "            with open(train_info_path + 'train_info.txt', 'a') as f:\n",
    "                f.write(info + '\\n')\n",
    "            print(info)\n",
    "            \n",
    "        # 衰减学习率\n",
    "        if (it + 1) % 4000 == 0:\n",
    "            lr_sche.step()\n",
    "#             lr_sche_lam.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./logger.npy\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158dd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = np.load(\"./logger.npy\", allow_pickle=True).item()\n",
    "k = 2\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    plt.subplot(111)\n",
    "    # plt.plot(logger[\"iter\"][::k], logger[\"loss\"][::k], label=r\"$L$\")\n",
    "    plt.plot(logger[\"iter\"][::k], logger[\"loss_res_1\"][::k], label=r\"$\\mathcal{L}_{r}$\", linewidth=3)\n",
    "    plt.plot(logger[\"iter\"][::k], logger[\"loss_ics\"][::k], label=r\"$\\mathcal{L}_{data}$\", linewidth=3)\n",
    "    plt.legend()\n",
    "    plt.xticks([0, 5000, 10000, 15000, 20000])\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('loss.png', dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c639cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模型参数\n",
    "mu = logger[\"mu\"].cpu()\n",
    "sigma = logger[\"sigma\"].cpu()\n",
    "\n",
    "backbone = MLP(backbone_layers)  # 主干网络\n",
    "pinn = PINN(backbone, mu, sigma)\n",
    "\n",
    "model_state = torch.load(os.path.join(model_path, 'pinn_adam.pth'))\n",
    "pinn.backbone.load_state_dict(model_state['backbone_state'])\n",
    "pinn.eval()\n",
    "\n",
    "# 生成网格\n",
    "t_res = np.linspace(tmin, tmax, 100)\n",
    "x_res = np.linspace(xmin, xmax, 100)\n",
    "X_res = dataset.sample_xy(t_res, x_res)\n",
    "\n",
    "X_res = np.expand_dims(X_res, axis=2)\n",
    "X_res = np.repeat(X_res,5,axis=2)\n",
    "print(X_res.shape)\n",
    "\n",
    "X_res = torch.from_numpy(X_res).double()\n",
    "pinn = pinn.cpu().double()\n",
    "X1_pred,X2_pred,X3_pred,X4_pred,X5_pred = pinn.net_X_res_pred(X_res)\n",
    "\n",
    "X1_pred = X1_pred.detach().numpy()\n",
    "X2_pred = X2_pred.detach().numpy()\n",
    "X3_pred = X3_pred.detach().numpy()\n",
    "X4_pred = X4_pred.detach().numpy()\n",
    "X5_pred = X5_pred.detach().numpy()\n",
    "\n",
    "print(X1_pred)\n",
    "print(t_res)\n",
    "# X1_error = np.linalg.norm(X1_pred - X1_star) / np.linalg.norm(X1_star) \n",
    "# print('Relative l2 error of u: {:.3e}'.format(u_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.rcParams.update({'font.size':18})\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig = plt.figure(figsize=(16, 5))\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    plt.plot(t_res, X3_pred[::100], linewidth=3)\n",
    "#     plt.scatter(X1_pred[:,0], X1_pred[:,1])\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('X1')\n",
    "#     plt.xticks([0, 1, 2, 3, 4])\n",
    "    plt.legend(loc='upper right', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    ax.set_aspect(1./ax.get_data_ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd0642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
