{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abd58bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2751755/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910f29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "# import imageio\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import pickle\n",
    "import scipy.io\n",
    "import random\n",
    "import math\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4773eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "class Options_AC(object):\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--use_subnet', default=True, help=' use subnet or not')\n",
    "        parser.add_argument('--no_cuda', action='store_true', default=False, help='disable CUDA or not')\n",
    "        parser.add_argument('--dim_hidden', type=int, default=20, help='neurons in hidden layers')\n",
    "#         parser.add_argument('--dim_hidden', type=int, default=128, help='neurons in hidden layers')\n",
    "        parser.add_argument('--hidden_layers', type=int, default=4, help='number of hidden layers')\n",
    "#         parser.add_argument('--hidden_layers', type=int, default=8, help='number of hidden layers')\n",
    "        parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
    "        parser.add_argument('--epochs_Adam', type=int, default=15000, help='epochs for Adam optimizer')\n",
    "#         parser.add_argument('--epochs_Adam', type=int, default=150000, help='epochs for Adam optimizer')\n",
    "        parser.add_argument('--epochs_LBFGS', type=int, default=1000, help='epochs for LBFGS optimizer')\n",
    "        parser.add_argument('--newton_iter', type=int, default=100, help='newton_iter for LBFGS optimizer')\n",
    "        parser.add_argument('--step_size', type=int, default=5000, help='step size in lr_scheduler for Adam optimizer')\n",
    "        parser.add_argument('--gamma', type=float, default=0.9, help='gamma in lr_scheduler for Adam optimizer')\n",
    "        \n",
    "        self.parser = parser\n",
    "\n",
    "    def parse(self):\n",
    "        arg = self.parser.parse_args(args=[])\n",
    "        # arg.cuda = False\n",
    "        # arg.device = torch.device('cpu')\n",
    "        arg.cuda = not arg.no_cuda and torch.cuda.is_available()\n",
    "        arg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        return arg\n",
    "\n",
    "args = Options_AC().parse()\n",
    "print(args.hidden_layers)\n",
    "\n",
    "def save_model(state, is_best=None, save_dir=None):\n",
    "    last_model = os.path.join(save_dir, f'last_model.pth')\n",
    "    torch.save(state, last_model)\n",
    "    if is_best:\n",
    "        best_model = os.path.join(save_dir, f'best_model.pth')\n",
    "#         shutil.copyfile(last_model, best_model)\n",
    "        shutil.copyfile(best_model, last_model)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d9b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\" compute the derivative of outputs associated with inputs\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "    outputs: (N, 1) tensor\n",
    "    inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True)\n",
    "\n",
    "def activation(name):\n",
    "    if name in ['tanh', 'Tanh']:\n",
    "        return nn.Tanh()\n",
    "    elif name in ['relu', 'ReLU']:\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif name in ['leaky_relu', 'LeakyReLU']:\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif name in ['sigmoid', 'Sigmoid']:\n",
    "        return nn.Sigmoid()\n",
    "    elif name in ['softplus', 'Softplus']:\n",
    "        return nn.Softplus()\n",
    "    else:\n",
    "        raise ValueError(f'unknown activation function: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6d5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638ebb10",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Modified_MLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden, hidden_layers,\n",
    "                 act_name='tanh', init_name='xavier_normal'):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        encoder_U = nn.Sequential()\n",
    "        encoder_U.add_module('fc', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        encoder_U.add_module('act', activation(act_name))\n",
    "        self.encoder_U = encoder_U\n",
    "\n",
    "        encoder_V = nn.Sequential()\n",
    "        encoder_V.add_module('fc', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        encoder_V.add_module('act', activation(act_name))\n",
    "        self.encoder_V = encoder_V\n",
    "\n",
    "        model = nn.Sequential()\n",
    "        model.add_module('fc0', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        model.add_module('act0', activation(act_name))\n",
    "        for i in range(1, hidden_layers):\n",
    "            model.add_module(f'fc{i}', nn.Linear(dim_hidden, dim_hidden, bias=True))\n",
    "            model.add_module(f'act{i}', activation(act_name))\n",
    "        model.add_module(f'fc{hidden_layers}', nn.Linear(dim_hidden, dim_out, bias=True))\n",
    "\n",
    "        self.model = model\n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "\n",
    "        self.size = self.model_size()\n",
    "\n",
    "    def init_weight(self, name):\n",
    "        \"\"\"初始化网络参数\"\"\"\n",
    "\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"模型的正向传播\"\"\"\n",
    "        U = self.encoder_U(x)\n",
    "        V = self.encoder_V(x)\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = self.model[2 * i](x)      # 调用线性层\n",
    "            x = self.model[2 * i + 1](x)  # 调用激活层\n",
    "            x = (1 - x) * U + x * V       # 特征融合\n",
    "        x = self.model[-1](x)             # 调用最后一个线性层得到输出\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        \"\"\"模型大小\"\"\"\n",
    "        n_params = 0\n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa55d5c",
   "metadata": {},
   "source": [
    "## 网络模型(DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d02638",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Deep Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, L, M, dim_hidden, hidden_layers, dim_out,\n",
    "                 act_name='tanh', init_name='xavier_normal'):\n",
    "        super().__init__()\n",
    "        \n",
    "        dim_in = M * 2 + 2\n",
    "        \n",
    "        model = Modified_MLP(dim_in, dim_out, dim_hidden, hidden_layers)\n",
    "        \n",
    "#         model = nn.Sequential()\n",
    "        \n",
    "#         model.add_module('fc0', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "#         model.add_module('act0', activation(act_name))\n",
    "#         for i in range(1, hidden_layers):\n",
    "#             model.add_module(f'fc{i}', nn.Linear(dim_hidden, dim_hidden, bias=True))\n",
    "#             model.add_module(f'act{i}', activation(act_name))\n",
    "#         model.add_module(f'fc{hidden_layers}', nn.Linear(dim_hidden, dim_out, bias=True))\n",
    "            \n",
    "        self.model = model\n",
    "        \n",
    "        self.L = L\n",
    "        self.M = M\n",
    "        \n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "\n",
    "            \n",
    "        self.k = nn.Parameter(torch.arange(1, self.M+1).float(), requires_grad=False)\n",
    "                    \n",
    "    def init_weight(self, name):\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "                \n",
    "    def input_encoding(self, t, x):\n",
    "        w = 2.0 * math.pi / self.L\n",
    "        out = torch.hstack([a]) \n",
    "        \n",
    "        return out    \n",
    "            \n",
    "    def forward(self, H):\n",
    "        t = H[:, 0:1]\n",
    "        x = H[:, 1:2]\n",
    "        y = H[:, 2:3]\n",
    "        \n",
    "        H = self.input_encoding(t, x, y)\n",
    "        H = self.model(H)\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        print(f\"{'input':<20}{str(x.shape):<40}\")\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            print(f\"{name:<20}{str(x.shape):<40}\")\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        n_params = 0\n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3996da",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    \"\"\"Deep Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden, hidden_layers,\n",
    "                 act_name='tanh', init_name=None):\n",
    "        super().__init__()\n",
    "        model = nn.Sequential()\n",
    "\n",
    "        model.add_module('fc0', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        model.add_module('act0', activation(act_name))\n",
    "        for i in range(1, hidden_layers):\n",
    "            model.add_module(f'fc{i}', nn.Linear(dim_hidden, dim_hidden, bias=True))\n",
    "            model.add_module(f'act{i}', activation(act_name))\n",
    "        model.add_module(f'fc{hidden_layers}', nn.Linear(dim_hidden, dim_out, bias=True))\n",
    "\n",
    "        self.model = model\n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "\n",
    "    def init_weight(self, name):\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def forward_test(self, x):\n",
    "        print(f\"{'input':<20}{str(x.shape):<40}\")\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            print(f\"{name:<20}{str(x.shape):<40}\")\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        n_params = 0\n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "864dea60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (model): Modified_MLP(\n",
       "    (encoder_U): Sequential(\n",
       "      (fc): Linear(in_features=22, out_features=20, bias=True)\n",
       "      (act): Tanh()\n",
       "    )\n",
       "    (encoder_V): Sequential(\n",
       "      (fc): Linear(in_features=22, out_features=20, bias=True)\n",
       "      (act): Tanh()\n",
       "    )\n",
       "    (model): Sequential(\n",
       "      (fc0): Linear(in_features=22, out_features=20, bias=True)\n",
       "      (act0): Tanh()\n",
       "      (fc1): Linear(in_features=20, out_features=20, bias=True)\n",
       "      (act1): Tanh()\n",
       "      (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
       "      (act2): Tanh()\n",
       "      (fc3): Linear(in_features=20, out_features=20, bias=True)\n",
       "      (act3): Tanh()\n",
       "      (fc4): Linear(in_features=20, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(L=2.0, M=10, dim_hidden=args.dim_hidden, hidden_layers=args.hidden_layers, dim_out=1)\n",
    "model.cuda()\n",
    "args.model=model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc4cea",
   "metadata": {},
   "source": [
    "## 数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccb8fc8a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Trainset_AC():\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        self.shape = (self.args[1], self.args[0])\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.data()\n",
    "    \n",
    "    def data(self):\n",
    "        \n",
    "        Nsd = self.args[0]\n",
    "        n_ics = self.args[1]\n",
    "        \n",
    "        lb = np.array([0.0, 0.0, 0.0])\n",
    "        ub = np.array([10.0, 1.0, 1.0])\n",
    "        tx = (ub-lb)*lhs(3, Nsd)+lb\n",
    "        tx_ad = tx[(tx[:,0]>0.1)&(tx[:,1]>-0.9)&(tx[:,1]<0.9)]\n",
    "        tx_no_ad = tx[~((tx[:,0]>0.1)&(tx[:,1]>-0.9)&(tx[:,1]<0.9))]\n",
    "        \n",
    "        lb_ics = np.array([0.0, 0.0, 0.0])\n",
    "        ub_ics = np.array([10.0, 1.0, 1.0]) \n",
    "        tx_ics = (ub_ics-lb_ics)*lhs(3, n_ics)+lb_ics\n",
    "        \n",
    "        u_ics = np.tanh((0.35-np.sqrt((tx[:,[1]]-0.5)**2 + (tx[:,[2]]-0.5)**2))/(2*0.025))\n",
    "        \n",
    "        \n",
    "        tx_ad = torch.from_numpy(tx_ad).float().cuda()\n",
    "        tx_no_ad = torch.from_numpy(tx_no_ad).float().cuda()\n",
    "        tx_ics = torch.from_numpy(tx_ics).float().cuda()\n",
    "        u_ics = torch.from_numpy(u_ics).float().cuda()\n",
    "        \n",
    "        return tx_ad, tx_no_ad, tx_ics, u_ics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae77854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22806, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = Trainset_AC(25600, 512)\n",
    "args.trainset = trainset\n",
    "tx_ad, tx_no_ad, tx_ics, u_ics = trainset()\n",
    "tx_ad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bc26c",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ad6614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(DNN):\n",
    "    \"\"\"Physics Constrained Neural Networks\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in=2, dim_out=1, dim_hidden=10, hidden_layers=5,\n",
    "                 act_name='sigmoid', init_name=None):\n",
    "        super().__init__(dim_in, dim_out, dim_hidden, hidden_layers,\n",
    "                         act_name=act_name, init_name=init_name)\n",
    "        self.model.fc5.bias.requires_grad = False\n",
    "    def minmaxscaler(self, data):\n",
    "        mmin = torch.min(data,dim=0)[0]\n",
    "        mmax = torch.max(data,dim=0)[0]\n",
    "        data = (data - mmin)/(mmax-mmin)\n",
    "        output = data/torch.sum(data) * len(data) + 1\n",
    "        \n",
    "        return output\n",
    "    def forward(self, x):\n",
    "        u = super().forward(x)\n",
    "        u = self.minmaxscaler(u)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc187e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_AC(object):\n",
    "    def __init__(self, args):\n",
    "        self.model = args.model\n",
    "        self.lr = args.lr\n",
    "        self.gamma = args.gamma\n",
    "\n",
    "        self.newton_iter = args.newton_iter\n",
    "        self.step_size = args.step_size\n",
    "        \n",
    "        self.model_name = self.model.__class__.__name__\n",
    "        self.model_path = self._model_path()\n",
    "        \n",
    "        self.epochs_Adam = args.epochs_Adam\n",
    "        self.epochs_LBFGS = args.epochs_LBFGS\n",
    "        self.optimizer_Adam = optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999))\n",
    "        self.optimizer_LBFGS = optim.LBFGS(self.model.parameters(),\n",
    "                                            lr=0.8,\n",
    "                                            max_iter=self.newton_iter,\n",
    "                                            tolerance_grad=1.e-5,\n",
    "                                            tolerance_change=1.e-9)\n",
    "        self.scheduler = lr_scheduler.ExponentialLR(self.optimizer_Adam, gamma=self.gamma, verbose=True)\n",
    "        \n",
    "        # subnet(Unet)\n",
    "        self.use_subnet = args.use_subnet\n",
    "        self.Unet = args.Unet\n",
    "        self.Unet.cuda()\n",
    "        self.Unet.zero_grad()\n",
    "        self.optimizer_Adam_Unet = optim.Adam(filter(lambda p: p.requires_grad==True, self.Unet.parameters()), lr=args.lr)    \n",
    "        self.scheduler_Unet = lr_scheduler.ExponentialLR(self.optimizer_Adam_Unet, gamma=self.gamma, verbose=True)\n",
    "        \n",
    "        # data\n",
    "        self.tx_ad, self.tx_no_ad, self.tx_ics, self.u_ics = args.trainset()\n",
    " \n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.loss_b_log = []\n",
    "        self.loss_r_log = []\n",
    "        self.epoch_log = []\n",
    "        \n",
    "        self.loss_lbfgs_log = []\n",
    "        self.loss_b_lbfgs_log = []\n",
    "        self.loss_r_lbfgs_log = []\n",
    "        self.epoch_lbfgs_log = []\n",
    "        \n",
    "        \n",
    "    def _model_path(self):\n",
    "        \"\"\"Path to save the model\"\"\"\n",
    "        if not os.path.exists('checkpoints'):\n",
    "            os.mkdir('checkpoints')\n",
    "        \n",
    "        path = os.path.join('checkpoints', self.model_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        return path\n",
    "\n",
    "    \n",
    "    def net_r(self, tx):\n",
    "        tx.requires_grad_(True)\n",
    "\n",
    "        u = self.model(tx)\n",
    "\n",
    "        grad_u = grad(u, tx)[0]\n",
    "        u_t = grad_u[:,[0]]\n",
    "        u_x = grad_u[:,[1]]\n",
    "        u_y = grad_u[:,[2]]\n",
    "        u_xx = grad(u_x, tx)[0][:,[1]]\n",
    "        u_yy = grad(u_y, tx)[0][:,[2]]\n",
    "\n",
    "        eps = 0.025\n",
    "        lam = 10.\n",
    "        \n",
    "        residual = u_t - (eps**2 * (u_xx + u_yy) - u**3 + u) * lam\n",
    "\n",
    "        return residual\n",
    "\n",
    "    \n",
    "    def net_u(self, tx):\n",
    "        u = self.model(tx)\n",
    "        return u\n",
    "    \n",
    "    \n",
    "    def loss(self, use_ad=False):\n",
    "        self.r_ad_pred = self.net_r(self.tx_ad)\n",
    "        self.r_no_ad_pred = self.net_r(self.tx_no_ad)\n",
    "        self.g_pred = self.net_u(self.tx_ics)\n",
    "\n",
    "        if use_ad:\n",
    "\n",
    "            self.r_weights = self.Unet(self.tx_ad)\n",
    "            loss_r = torch.mean((self.r_weights * self.r_ad_pred)**2) + torch.mean(self.r_no_ad_pred**2)\n",
    "            loss_b = torch.mean((self.g_pred-self.u_ics)**2)\n",
    "            loss = loss_r + 100*loss_b\n",
    "\n",
    "        else:\n",
    "            loss_r = torch.mean(self.r_ad_pred**2) + torch.mean(self.r_no_ad_pred**2)\n",
    "            loss_b = torch.mean((self.g_pred-self.u_ics)**2)\n",
    "            loss = loss_r + 100*loss_b\n",
    "\n",
    "        return loss, loss_r, loss_b\n",
    "\n",
    "\n",
    "    def train_Adam(self, mode='0'):\n",
    "        \"\"\"\n",
    "        mode='0'：subnet参与计算，只训练主网络\n",
    "        mode='1'：subnet参与计算，只训练subnet\n",
    "        mode='2'：subnet参与计算，主网络和subnet对抗训练\n",
    "        mode='3'：subnet不参与计算，只训练主网络\n",
    "        \"\"\"\n",
    "        if mode == '0':\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss(use_ad=True)\n",
    "            loss_value.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "\n",
    "            return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "        \n",
    "        elif mode == '1':\n",
    "            self.optimizer_Adam_Unet.zero_grad()\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss(use_ad=True)\n",
    "            loss_value.backward()\n",
    "\n",
    "            for name, parms in self.Unet.named_parameters():\n",
    "                if parms.requires_grad==True:\n",
    "                    parms.grad *= -1\n",
    "\n",
    "            self.optimizer_Adam_Unet.step()        \n",
    "\n",
    "            return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "\n",
    "        elif mode == '2':\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            self.optimizer_Adam_Unet.zero_grad()\n",
    "\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss(use_ad=True)\n",
    "            loss_value.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "\n",
    "            for name, parms in self.Unet.named_parameters():\n",
    "                if parms.requires_grad==True:\n",
    "                    parms.grad *= -1\n",
    "\n",
    "            self.optimizer_Adam_Unet.step()\n",
    "\n",
    "            return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "\n",
    "        else:\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss(use_ad=False)\n",
    "            loss_value.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "\n",
    "            return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "        \n",
    "\n",
    "    def train_LBFGS(self):\n",
    "        loss_value, loss_r_value, loss_b_value = self.loss()\n",
    "        \n",
    "        def closure():\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss()\n",
    "\n",
    "            self.optimizer_LBFGS.zero_grad()\n",
    "            loss_value.backward()\n",
    "\n",
    "            return loss_value\n",
    "\n",
    "        self.optimizer_LBFGS.step(closure)\n",
    "        loss_value = closure()\n",
    "        \n",
    "        return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "    \n",
    "    \n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        loss_value, loss_r_value, loss_b_value = self.loss()\n",
    "        \n",
    "        infos = 'Valid   ' + \\\n",
    "                f'Loss:{loss_value:.4e}  ' + \\\n",
    "                f'Loss_r:{loss_r_value:.4e} '\n",
    "        print(infos)\n",
    "        self.model.train()\n",
    "        return loss_value.item()\n",
    "    \n",
    "    \n",
    "    def save_both_model(self, step, epoch):\n",
    "        # save backbone\n",
    "        model_state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "        }\n",
    "        backbone = os.path.join(self.model_path, 'backbone_%s_%d.pth' % (step, epoch))\n",
    "        torch.save(model_state, backbone)\n",
    "\n",
    "        # save subnet\n",
    "        subnet_state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.Unet.state_dict(),\n",
    "        }    \n",
    "        subnet = os.path.join(self.model_path, 'subnet_%s_%d.pth' % (step, epoch))\n",
    "        torch.save(subnet_state, subnet)\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        best_loss = 1.e10\n",
    "        \n",
    "        # 1.只训练主网络\n",
    "        for epoch in range(5000):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_Adam(mode='3')    \n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)\n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)\n",
    "                \n",
    "            if (epoch+1) % 1000 == 0:\n",
    "                self.save_both_model('step1', epoch+1)\n",
    "                                \n",
    "                # data\n",
    "                self.tx_ad, self.tx_no_ad, self.tx_ics, self.u_ics = args.trainset()\n",
    "                \n",
    "        # 2.只训练subnet      \n",
    "        for epoch in range(2000):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_Adam(mode='1')      \n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)\n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)\n",
    "                \n",
    "            if (epoch+1) % 1000 == 0:\n",
    "                self.save_both_model('step2', epoch+1)\n",
    "                                \n",
    "                # data\n",
    "                self.tx_ad, self.tx_no_ad, self.tx_ics, self.u_ics = args.trainset()\n",
    "                \n",
    "                \n",
    "        # 3.主网络和subnet对抗，一起训练      \n",
    "        for epoch in range(self.epochs_Adam):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_Adam(mode='2')\n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)\n",
    "            \n",
    "            #学习率衰减？\n",
    "            if (epoch+1) % self.step_size == 0:\n",
    "                self.scheduler.step()\n",
    "                self.scheduler_Unet.step()\n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)  \n",
    "                \n",
    "            if (epoch+1) % 1000 == 0:\n",
    "                self.save_both_model('step3', epoch+1)\n",
    "                                \n",
    "                # data\n",
    "                self.tx_ad, self.tx_no_ad, self.tx_ics, self.u_ics = args.trainset()\n",
    "         \n",
    "        \n",
    "        # 使用最好的模型\n",
    "        state_dict = torch.load(f'{self.model_path}/best_model.pth')\n",
    "        self.model.load_state_dict(state_dict['state_dict'])\n",
    "            \n",
    "        # 4.只训练主网络主网络\n",
    "        for epoch in range(self.epochs_Adam):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_Adam(mode='3')\n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)\n",
    "            \n",
    "            #学习率衰减？\n",
    "            if (epoch+1) % self.step_size == 0:\n",
    "                self.scheduler.step()         \n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)  \n",
    "                \n",
    "            if (epoch+1) % 1000 == 0:\n",
    "                self.save_both_model('step4', epoch+1) \n",
    "                \n",
    "                # data\n",
    "                self.tx_ad, self.tx_no_ad, self.tx_ics, self.u_ics = args.trainset()\n",
    "\n",
    "        # 5.lbfgs只训练主网络主网络    \n",
    "        for epoch in range(self.epochs_LBFGS):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_LBFGS()\n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)         \n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam + self.epochs_LBFGS}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)\n",
    "                \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                self.save_both_model('step5', epoch+1) \n",
    "                \n",
    "                # data\n",
    "                self.tx_ad, self.tx_no_ad, self.tx_ics, self.u_ics = args.trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46fb8fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.78 GiB total capacity; 261.22 MiB already allocated; 12.94 MiB free; 270.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m args\u001b[38;5;241m.\u001b[39mUnet \u001b[38;5;241m=\u001b[39m Unet(dim_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer_AC(args)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [13], line 211\u001b[0m, in \u001b[0;36mTrainer_AC.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# 1.只训练主网络\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5000\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     loss_value, loss_r_value, loss_b_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_log\u001b[38;5;241m.\u001b[39mappend(loss_value)\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_r_log\u001b[38;5;241m.\u001b[39mappend(loss_r_value)\n",
      "Cell \u001b[0;32mIn [13], line 151\u001b[0m, in \u001b[0;36mTrainer_AC.train_Adam\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_Adam\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 151\u001b[0m     loss_value, loss_r_value, loss_b_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_ad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     loss_value\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_Adam\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn [13], line 85\u001b[0m, in \u001b[0;36mTrainer_AC.loss\u001b[0;34m(self, use_ad)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, use_ad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_ad_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_r(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtx_ad)\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_no_ad_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_r\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtx_no_ad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_u(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtx_ics)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_ad:\n",
      "Cell \u001b[0;32mIn [13], line 67\u001b[0m, in \u001b[0;36mTrainer_AC.net_r\u001b[0;34m(self, tx)\u001b[0m\n\u001b[1;32m     65\u001b[0m u_x \u001b[38;5;241m=\u001b[39m grad_u[:,[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     66\u001b[0m u_y \u001b[38;5;241m=\u001b[39m grad_u[:,[\u001b[38;5;241m2\u001b[39m]]\n\u001b[0;32m---> 67\u001b[0m u_xx \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:,[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     68\u001b[0m u_yy \u001b[38;5;241m=\u001b[39m grad(u_y, tx)[\u001b[38;5;241m0\u001b[39m][:,[\u001b[38;5;241m2\u001b[39m]]\n\u001b[1;32m     70\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.025\u001b[39m\n",
      "Cell \u001b[0;32mIn [4], line 20\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad\u001b[39m(outputs, inputs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124;03m\"\"\" compute the derivative of outputs associated with inputs\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Params\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    inputs: (N, D) tensor\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch39/lib/python3.9/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.78 GiB total capacity; 261.22 MiB already allocated; 12.94 MiB free; 270.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "args.Unet = Unet(dim_in=2)\n",
    "trainer = Trainer_AC(args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最好的模型\n",
    "state_dict = torch.load(f'{trainer.model_path}/best_model.pth')\n",
    "model.load_state_dict(state_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f'checkpoints/subnet,{args.epochs_Adam}Adam,{args.epochs_LBFGS}LBFGS'\n",
    "if os.path.exists(file)==False:\n",
    "    os.mkdir(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca0796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f'{file}/loss_r_log.npy',np.array(trainer.loss_r_log))\n",
    "# np.save(f'{file}/loss_b_log.npy',np.array(trainer.loss_b_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.loss_r_log = np.load(f'{file}/loss_r_log.npy')\n",
    "trainer.loss_b_log = np.load(f'{file}/loss_b_log.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caac180",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    plt.rcParams.update({'font.size':16})\n",
    "    plt.plot(np.arange(len(trainer.loss_r_log)), trainer.loss_r_log, label='$ \\mathcal{L}_r$')\n",
    "    plt.plot(np.arange(len(trainer.loss_b_log)), trainer.loss_b_log, label='$ \\mathcal{L}_{ic}$')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{file}/loss.png', dpi=750, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0da72d",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ade42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu();\n",
    "trainer.Unet.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ffcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = scipy.io.loadmat('AC.mat')\n",
    "usol = data['uu']\n",
    "\n",
    "# Grid\n",
    "t_star = data['tt'][0]\n",
    "x_star = data['x'][0]\n",
    "TT, XX = np.meshgrid(t_star, x_star)\n",
    "\n",
    "# Reference solution\n",
    "plt.pcolor(TT, XX, usol, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2808d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained network parameters\n",
    "TX = np.hstack((TT.reshape(-1,1), XX.reshape(-1,1)))\n",
    "TX = torch.from_numpy(TX).double()\n",
    "\n",
    "model = trainer.model.cpu().double()\n",
    "u_pred = model(TX).detach().numpy()\n",
    "u_pred = u_pred.reshape(TT.shape)\n",
    "\n",
    "error = np.linalg.norm(u_pred - usol) / np.linalg.norm(usol) \n",
    "print('Relative l2 error: {:.3e}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed154d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':18})\n",
    "\n",
    "fig = plt.figure(4, figsize=(24, 5))\n",
    "ax1 = plt.subplot(1, 4, 1)\n",
    "plt.pcolor(TT, XX, usol, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title(r'Reference $u(t,x)$')\n",
    "plt.tight_layout()\n",
    "ax1.set_aspect(1./ax1.get_data_ratio())\n",
    "\n",
    "ax2 = plt.subplot(1, 4, 2)\n",
    "plt.pcolor(TT, XX, u_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title(r'Predicted $u(t,x)$')\n",
    "plt.tight_layout()\n",
    "ax2.set_aspect(1./ax2.get_data_ratio())\n",
    "\n",
    "ax3 = plt.subplot(1, 4, 3)\n",
    "plt.pcolor(TT, XX, np.abs(usol - u_pred), cmap='jet')\n",
    "cbar = plt.colorbar()\n",
    "# cbar.formatter.set_powerlimits((0,0))\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title(r'Absolute error')\n",
    "plt.tight_layout()\n",
    "ax3.set_aspect(1./ax3.get_data_ratio())\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    ax4 = plt.subplot(1, 4, 4)\n",
    "    plt.plot(np.arange(len(trainer.loss_r_log)), trainer.loss_r_log, label='$ \\mathcal{L}_r$')\n",
    "    plt.plot(np.arange(len(trainer.loss_b_log)), trainer.loss_b_log, label='$ \\mathcal{L}_{ic}$')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    ax4.set_aspect(1./ax4.get_data_ratio())\n",
    "\n",
    "\n",
    "plt.savefig(f'{file}/results_ac.png', dpi=400, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
