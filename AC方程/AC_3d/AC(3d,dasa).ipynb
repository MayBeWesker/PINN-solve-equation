{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf37ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "# import imageio\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import pickle\n",
    "import scipy.io\n",
    "import random\n",
    "import math\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820c4550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "class Options_AC(object):\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--use_subnet', default=True, help=' use subnet or not')\n",
    "        parser.add_argument('--no_cuda', action='store_true', default=False, help='disable CUDA or not')\n",
    "        parser.add_argument('--dim_hidden', type=int, default=128, help='neurons in hidden layers')\n",
    "        parser.add_argument('--hidden_layers', type=int, default=6, help='number of hidden layers')\n",
    "        parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
    "        parser.add_argument('--epochs_Adam', type=int, default=100000, help='epochs for Adam optimizer')\n",
    "        parser.add_argument('--epochs_LBFGS', type=int, default=0, help='epochs for LBFGS optimizer')\n",
    "        parser.add_argument('--newton_iter', type=int, default=100, help='newton_iter for LBFGS optimizer')\n",
    "        parser.add_argument('--step_size', type=int, default=5000, help='step size in lr_scheduler for Adam optimizer')\n",
    "        parser.add_argument('--gamma', type=float, default=0.9, help='gamma in lr_scheduler for Adam optimizer')\n",
    "        \n",
    "        self.parser = parser\n",
    "\n",
    "    def parse(self):\n",
    "        arg = self.parser.parse_args(args=[])\n",
    "        # arg.cuda = False\n",
    "        # arg.device = torch.device('cpu')\n",
    "        arg.cuda = not arg.no_cuda and torch.cuda.is_available()\n",
    "        arg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        return arg\n",
    "\n",
    "args = Options_AC().parse()\n",
    "print(args.hidden_layers)\n",
    "\n",
    "def save_model(state, is_best=None, save_dir=None):\n",
    "    last_model = os.path.join(save_dir, 'last_model.pth')\n",
    "    torch.save(state, last_model)\n",
    "    if is_best:\n",
    "        best_model = os.path.join(save_dir, 'best_model.pth')\n",
    "        shutil.copyfile(last_model, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435bb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\" compute the derivative of outputs associated with inputs\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "    outputs: (N, 1) tensor\n",
    "    inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True)\n",
    "\n",
    "def activation(name):\n",
    "    if name in ['tanh', 'Tanh']:\n",
    "        return nn.Tanh()\n",
    "    elif name in ['relu', 'ReLU']:\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif name in ['leaky_relu', 'LeakyReLU']:\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif name in ['sigmoid', 'Sigmoid']:\n",
    "        return nn.Sigmoid()\n",
    "    elif name in ['softplus', 'Softplus']:\n",
    "        return nn.Softplus()\n",
    "    else:\n",
    "        raise ValueError(f'unknown activation function: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f314874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70fb3d2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Modified_MLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden, hidden_layers,\n",
    "                 act_name='tanh', init_name='xavier_normal'):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        encoder_U = nn.Sequential()\n",
    "        encoder_U.add_module('fc', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        encoder_U.add_module('act', activation(act_name))\n",
    "        self.encoder_U = encoder_U\n",
    "\n",
    "        encoder_V = nn.Sequential()\n",
    "        encoder_V.add_module('fc', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        encoder_V.add_module('act', activation(act_name))\n",
    "        self.encoder_V = encoder_V\n",
    "\n",
    "        model = nn.Sequential()\n",
    "        model.add_module('fc0', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        model.add_module('act0', activation(act_name))\n",
    "        for i in range(1, hidden_layers):\n",
    "            model.add_module(f'fc{i}', nn.Linear(dim_hidden, dim_hidden, bias=True))\n",
    "            model.add_module(f'act{i}', activation(act_name))\n",
    "        model.add_module(f'fc{hidden_layers}', nn.Linear(dim_hidden, dim_out, bias=True))\n",
    "\n",
    "        self.model = model\n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "\n",
    "        self.size = self.model_size()\n",
    "\n",
    "    def init_weight(self, name):\n",
    "        \"\"\"初始化网络参数\"\"\"\n",
    "\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"模型的正向传播\"\"\"\n",
    "        U = self.encoder_U(x)\n",
    "        V = self.encoder_V(x)\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = self.model[2 * i](x)      # 调用线性层\n",
    "            x = self.model[2 * i + 1](x)  # 调用激活层\n",
    "            x = (1 - x) * U + x * V       # 特征融合\n",
    "        x = self.model[-1](x)             # 调用最后一个线性层得到输出\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        \"\"\"模型大小\"\"\"\n",
    "        n_params = 0\n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8ca0b",
   "metadata": {},
   "source": [
    "## 网络模型(DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5db8cc6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Deep Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, L_x, L_y, M_t, M_x, M_y, dim_hidden, hidden_layers, dim_out,\n",
    "                 act_name='tanh', init_name='xavier_normal'):\n",
    "        super().__init__()\n",
    "        \n",
    "#         dim_in = M_x * M_y * 4 + (M_x+M_y) * 2 + (M_t+1) + 1\n",
    "        dim_in = (M_x+M_y) * 2 + (M_t+1) + 1\n",
    "        \n",
    "        model = DNN(dim_in, dim_out, dim_hidden, hidden_layers)\n",
    "        self.model = model\n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "            \n",
    "        self.M_t = M_t\n",
    "        self.M_x = M_x\n",
    "        self.M_y = M_y\n",
    "\n",
    "        self.w_x = 2.0 * math.pi / L_x\n",
    "        self.w_y = 2.0 * math.pi / L_y\n",
    "        \n",
    "        self.k_t = 10.0**torch.arange(0, M_t + 1)\n",
    "        self.k_x = torch.arange(1, M_x+1).float()\n",
    "        self.k_y = torch.arange(1, M_y+1).float()\n",
    "        \n",
    "        self.k_xx, self.k_yy = torch.meshgrid(self.k_x, self.k_y)\n",
    "        self.k_xx = self.k_xx.reshape(-1)\n",
    "        self.k_yy = self.k_yy.reshape(-1)\n",
    "        \n",
    "        self.k_t = nn.Parameter(self.k_t.float())\n",
    "        self.k_x = nn.Parameter(self.k_x.float())\n",
    "        self.k_y = nn.Parameter(self.k_y.float())\n",
    "        self.k_xx = nn.Parameter(self.k_xx.float())\n",
    "        self.k_yy = nn.Parameter(self.k_yy.float())\n",
    "        \n",
    "        \n",
    "    def init_weight(self, name):\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "                \n",
    "                           \n",
    "    def input_encoding(self, t, x, y):\n",
    "#         out = torch.hstack([torch.ones_like(t), self.k_t*t,\n",
    "#                             torch.cos(self.k_x * self.w_x * x), torch.cos(self.k_y * self.w_y * y),\n",
    "#                             torch.sin(self.k_x * self.w_x * x), torch.sin(self.k_y * self.w_y * y),\n",
    "#                             torch.cos(self.k_xx * self.w_x * x) * torch.cos(self.k_yy * self.w_y * y),\n",
    "#                             torch.cos(self.k_xx * self.w_x * x) * torch.sin(self.k_yy * self.w_y * y),\n",
    "#                             torch.sin(self.k_xx * self.w_x * x) * torch.cos(self.k_yy * self.w_y * y),\n",
    "#                             torch.sin(self.k_xx * self.w_x * x) * torch.sin(self.k_yy * self.w_y * y)])\n",
    "        \n",
    "        out = torch.hstack([torch.ones_like(t), self.k_t*t,\n",
    "                            torch.cos(self.k_x * self.w_x * x), torch.cos(self.k_y * self.w_y * y),\n",
    "                            torch.sin(self.k_x * self.w_x * x), torch.sin(self.k_y * self.w_y * y)])\n",
    "        \n",
    "        return out    \n",
    "            \n",
    "                           \n",
    "    def forward(self, H):\n",
    "        t = H[:, 0:1]\n",
    "        x = H[:, 1:2]\n",
    "        y = H[:, 2:3]\n",
    "    \n",
    "        # embedding\n",
    "        H = self.input_encoding(t, x, y)\n",
    "        \n",
    "        # 特征融合\n",
    "        H = self.model(H)\n",
    "        return H\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        print(f\"{'input':<20}{str(x.shape):<40}\")\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            print(f\"{name:<20}{str(x.shape):<40}\")\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        n_params = 0\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15642619",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    \"\"\"Deep Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden, hidden_layers,\n",
    "                 act_name='tanh', init_name=None):\n",
    "        super().__init__()\n",
    "        model = nn.Sequential()\n",
    "\n",
    "        model.add_module('fc0', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        model.add_module('act0', activation(act_name))\n",
    "        for i in range(1, hidden_layers):\n",
    "            model.add_module(f'fc{i}', nn.Linear(dim_hidden, dim_hidden, bias=True))\n",
    "            model.add_module(f'act{i}', activation(act_name))\n",
    "        model.add_module(f'fc{hidden_layers}', nn.Linear(dim_hidden, dim_out, bias=True))\n",
    "\n",
    "        self.model = model\n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "\n",
    "    def init_weight(self, name):\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def forward_test(self, x):\n",
    "        print(f\"{'input':<20}{str(x.shape):<40}\")\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            print(f\"{name:<20}{str(x.shape):<40}\")\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        n_params = 0\n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0994fe10",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a910ec2bc4cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model = MLP(L_x=1, L_y=1, M_t=2, M_x=5, M_y=5, dim_hidden=128, hidden_layers=6, dim_out=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         \"\"\"\n\u001b[1;32m--> 747\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         \"\"\"\n\u001b[1;32m--> 747\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# model = MLP(L_x=1, L_y=1, M_t=2, M_x=5, M_y=5, dim_hidden=128, hidden_layers=6, dim_out=1)\n",
    "model = DNN(dim_in=4, dim_out=1, dim_hidden=128, hidden_layers=6)\n",
    "model.cuda()\n",
    "args.model=model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ab792",
   "metadata": {},
   "source": [
    "## 数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a133988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainset_AC():\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        self.shape = (self.args[1], self.args[0])\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.data()\n",
    "    \n",
    "    def data(self):\n",
    "        \n",
    "        Nsd = self.args[0]\n",
    "        n_ics = self.args[1]\n",
    "        \n",
    "        lb = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "        ub = np.array([1., 1., 1., 1.])\n",
    "        txy = (ub-lb)*lhs(4, Nsd)+lb\n",
    "        \n",
    "        lb_ics = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "        ub_ics = np.array([0.0, 1., 1., 1.]) \n",
    "        txy_ics = (ub_ics-lb_ics)*lhs(4, n_ics)+lb_ics\n",
    "        \n",
    "        u_ics = np.tanh((0.35-np.sqrt((txy_ics[:,[1]]-0.5)**2 + (txy_ics[:,[2]]-0.5)**2) + (txy_ics[:,[3]]-0.5)**2)/(2*0.05))\n",
    "        \n",
    "        txy = torch.from_numpy(txy).float().cuda()\n",
    "        txy_ics = torch.from_numpy(txy_ics).float().cuda()\n",
    "        u_ics = torch.from_numpy(u_ics).float().cuda()\n",
    "        \n",
    "        return txy, txy_ics, u_ics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd0239a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-905b94e7ae85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainset_AC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtxy_ics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_ics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-bcc46ff7c651>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-bcc46ff7c651>\u001b[0m in \u001b[0;36mdata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mu_ics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.35\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxy_ics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtxy_ics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtxy_ics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mtxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mtxy_ics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxy_ics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mu_ics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_ics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "trainset = Trainset_AC(25600, 1024)\n",
    "args.trainset = trainset\n",
    "txy, txy_ics, u_ics = trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41d940",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(DNN):\n",
    "    \"\"\"Physics Constrained Neural Networks\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in=4, dim_out=1, dim_hidden=20, hidden_layers=5,\n",
    "                 act_name='sigmoid', init_name=None):\n",
    "        super().__init__(dim_in, dim_out, dim_hidden, hidden_layers,\n",
    "                         act_name=act_name, init_name=init_name)\n",
    "        self.model.fc5.bias.requires_grad = False\n",
    "    def minmaxscaler(self, data):\n",
    "        mmin = torch.min(data,dim=0)[0]\n",
    "        mmax = torch.max(data,dim=0)[0]\n",
    "        data = (data - mmin)/(mmax-mmin)\n",
    "        output = data/torch.sum(data) * len(data)\n",
    "        \n",
    "        return output\n",
    "    def forward(self, x):\n",
    "        u = super().forward(x)\n",
    "        u = self.minmaxscaler(u)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe690ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_AC(object):\n",
    "    def __init__(self, args):\n",
    "        self.model = args.model\n",
    "        self.lr = args.lr\n",
    "        self.gamma = args.gamma\n",
    "\n",
    "        self.newton_iter = args.newton_iter\n",
    "        self.step_size = args.step_size\n",
    "        \n",
    "        self.model_name = self.model.__class__.__name__\n",
    "        self.model_path = self._model_path()\n",
    "        \n",
    "        self.epochs_Adam = args.epochs_Adam\n",
    "        self.epochs_LBFGS = args.epochs_LBFGS\n",
    "        self.optimizer_Adam = optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999))\n",
    "        self.optimizer_LBFGS = optim.LBFGS(self.model.parameters(),\n",
    "                                            lr=0.8,\n",
    "                                            max_iter=self.newton_iter,\n",
    "                                            tolerance_grad=1.e-5,\n",
    "                                            tolerance_change=1.e-9)\n",
    "        self.scheduler = lr_scheduler.ExponentialLR(self.optimizer_Adam, gamma=self.gamma, verbose=True)\n",
    "        \n",
    "        # subnet(Unet)\n",
    "        self.use_subnet = args.use_subnet\n",
    "        self.Unet = args.Unet\n",
    "        self.Unet.cuda()\n",
    "        self.Unet.zero_grad()\n",
    "        self.optimizer_Adam_Unet = optim.Adam(filter(lambda p: p.requires_grad==True, self.Unet.parameters()), lr=args.lr)    \n",
    "        self.scheduler_Unet = lr_scheduler.ExponentialLR(self.optimizer_Adam_Unet, gamma=self.gamma, verbose=True)\n",
    "        \n",
    "        # data\n",
    "        self.txy, self.txy_ics, self.u_ics = args.trainset()\n",
    " \n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.loss_b_log = []\n",
    "        self.loss_r_log = []\n",
    "        self.epoch_log = []\n",
    "        \n",
    "        self.loss_lbfgs_log = []\n",
    "        self.loss_b_lbfgs_log = []\n",
    "        self.loss_r_lbfgs_log = []\n",
    "        self.epoch_lbfgs_log = []\n",
    "        \n",
    "        \n",
    "    def _model_path(self):\n",
    "        \"\"\"Path to save the model\"\"\"\n",
    "        if not os.path.exists('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        \n",
    "        path = os.path.join('checkpoint', self.model_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        return path\n",
    "\n",
    "    \n",
    "    def net_r(self, txy):\n",
    "        txy.requires_grad_(True)\n",
    "\n",
    "        u = self.model(txy)\n",
    "\n",
    "        grad_u = grad(u, txy)[0]\n",
    "        u_t = grad_u[:,[0]]\n",
    "        \n",
    "        u_x = grad_u[:,[1]]\n",
    "        u_y = grad_u[:,[2]]\n",
    "        \n",
    "        u_xx = grad(u_x, txy)[0][:,[1]]\n",
    "        u_yy = grad(u_y, txy)[0][:,[2]]\n",
    "        \n",
    "        eps = 0.05\n",
    "        lam = 10.\n",
    "        \n",
    "        residual = u_t - lam * (eps**2 * (u_xx + u_yy) - u**3 + u)\n",
    "\n",
    "        return residual\n",
    "\n",
    "    \n",
    "    def net_u(self, txy):\n",
    "        u = self.model(txy)\n",
    "        \n",
    "        return u\n",
    "    \n",
    "    \n",
    "    def loss(self, use_ad=False):\n",
    "        self.r_pred = self.net_r(self.txy)\n",
    "        self.g_pred = self.net_u(self.txy_ics)\n",
    "\n",
    "        if use_ad:\n",
    "\n",
    "            self.r_weights = self.Unet(self.txy)\n",
    "            loss_r = torch.mean(self.r_weights * self.r_pred**2)\n",
    "            loss_b = torch.mean((self.g_pred-self.u_ics)**2)\n",
    "            loss = loss_r + 100*loss_b\n",
    "\n",
    "        else:\n",
    "            loss_r = torch.mean(self.r_pred**2)\n",
    "            loss_b = torch.mean((self.g_pred-self.u_ics)**2)\n",
    "            loss = loss_r + 100*loss_b\n",
    "\n",
    "        return loss, loss_r, loss_b\n",
    "\n",
    "\n",
    "    def train_Adam(self, mode='0'):\n",
    "        \"\"\"\n",
    "        mode='0'：subnet参与计算，只训练主网络\n",
    "        mode='1'：subnet参与计算，只训练subnet\n",
    "        mode='2'：subnet参与计算，主网络和subnet对抗训练\n",
    "        mode='3'：subnet不参与计算，只训练主网络\n",
    "        \"\"\"\n",
    "        if mode == '0':\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss(use_ad=True)\n",
    "            loss_value.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "\n",
    "            return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "        \n",
    "        elif mode == '1':\n",
    "            self.optimizer_Adam_Unet.zero_grad()\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss(use_ad=True)\n",
    "            loss_value.backward()\n",
    "\n",
    "            for name, parms in self.Unet.named_parameters():\n",
    "                if parms.requires_grad==True:\n",
    "                    parms.grad *= -1\n",
    "\n",
    "            self.optimizer_Adam_Unet.step()        \n",
    "\n",
    "            return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "\n",
    "        elif mode == '2':\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            self.optimizer_Adam_Unet.zero_grad()\n",
    "\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss(use_ad=True)\n",
    "            loss_value.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "\n",
    "            for name, parms in self.Unet.named_parameters():\n",
    "                if parms.requires_grad==True:\n",
    "                    parms.grad *= -1\n",
    "\n",
    "            self.optimizer_Adam_Unet.step()\n",
    "\n",
    "            return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "\n",
    "        else:\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss(use_ad=False)\n",
    "            loss_value.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "\n",
    "            return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "        \n",
    "\n",
    "    def train_LBFGS(self):\n",
    "        loss_value, loss_r_value, loss_b_value = self.loss()\n",
    "        \n",
    "        def closure():\n",
    "            loss_value, loss_r_value, loss_b_value = self.loss()\n",
    "\n",
    "            self.optimizer_LBFGS.zero_grad()\n",
    "            loss_value.backward()\n",
    "\n",
    "            return loss_value\n",
    "\n",
    "        self.optimizer_LBFGS.step(closure)\n",
    "        loss_value = closure()\n",
    "        \n",
    "        return loss_value.item(), loss_r_value.item(), loss_b_value.item()\n",
    "    \n",
    "    \n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        loss_value, loss_r_value, loss_b_value = self.loss()\n",
    "        \n",
    "        infos = 'Valid   ' + \\\n",
    "                f'Loss:{loss_value:.4e}  ' + \\\n",
    "                f'Loss_r:{loss_r_value:.4e} '\n",
    "        print(infos)\n",
    "        self.model.train()\n",
    "        return loss_value.item()\n",
    "    \n",
    "    \n",
    "    def save_both_model(self, step, epoch):\n",
    "        # save backbone\n",
    "        model_state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "        }\n",
    "        backbone = os.path.join(self.model_path, 'backbone_%s_%d.pth' % (step, epoch))\n",
    "        torch.save(model_state, backbone)\n",
    "\n",
    "        # save subnet\n",
    "        subnet_state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.Unet.state_dict(),\n",
    "        }    \n",
    "        subnet = os.path.join(self.model_path, 'subnet_%s_%d.pth' % (step, epoch))\n",
    "        torch.save(subnet_state, subnet)\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        best_loss = 1.e10\n",
    "        \n",
    "        # 1.只训练主网络\n",
    "        for epoch in range(10000):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_Adam(mode='3')    \n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)\n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)\n",
    "                \n",
    "            if (epoch+1) % 100 == 0:          \n",
    "                # data\n",
    "                self.txy, self.txy_ics, self.u_ics = args.trainset()\n",
    "                \n",
    "        # 2.只训练subnet      \n",
    "        for epoch in range(5000):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_Adam(mode='1')      \n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)\n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)\n",
    "                \n",
    "            if (epoch+1) % 100 == 0:      \n",
    "                # data\n",
    "                self.txy, self.txy_ics, self.u_ics = args.trainset()\n",
    "                \n",
    "                \n",
    "        # 3.主网络和subnet对抗，一起训练      \n",
    "        for epoch in range(self.epochs_Adam):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_Adam(mode='2')\n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)\n",
    "            \n",
    "            \n",
    "            if (epoch+1) % self.step_size == 0:\n",
    "                self.scheduler.step()\n",
    "                self.scheduler_Unet.step()\n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)  \n",
    "                \n",
    "            if (epoch+1) % 100 == 0:      \n",
    "                # data\n",
    "                self.txy, self.txy_ics, self.u_ics = args.trainset()\n",
    "         \n",
    "        \n",
    "        # 使用最好的模型\n",
    "        state_dict = torch.load(f'{self.model_path}/best_model.pth')\n",
    "        self.model.load_state_dict(state_dict['state_dict'])\n",
    "            \n",
    "        # 4.只训练主网络主网络\n",
    "        for epoch in range(self.epochs_Adam):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_Adam(mode='3')\n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)\n",
    "            \n",
    "            \n",
    "            if (epoch+1) % self.step_size == 0:\n",
    "                self.scheduler.step()         \n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)  \n",
    "                \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                # data\n",
    "                self.txy, self.txy_ics, self.u_ics = args.trainset()\n",
    "\n",
    "        # 5.lbfgs只训练主网络主网络    \n",
    "        for epoch in range(self.epochs_LBFGS):\n",
    "            loss_value, loss_r_value, loss_b_value = self.train_LBFGS()\n",
    "            \n",
    "            self.loss_log.append(loss_value)\n",
    "            self.loss_r_log.append(loss_r_value)\n",
    "            self.loss_b_log.append(loss_b_value)\n",
    "            self.epoch_log.append(epoch)         \n",
    "            \n",
    "            if (epoch+1) % 1 == 0:\n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam + self.epochs_LBFGS}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_b:{loss_b_value:.2e},')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)\n",
    "                \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                # data\n",
    "                self.txy, self.txy_ics, self.u_ics = args.trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3cfa7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args.Unet = Unet(dim_in=4)\n",
    "trainer = Trainer_AC(args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68838c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最好的模型\n",
    "state_dict = torch.load(f'{trainer.model_path}/best_model.pth')\n",
    "model.load_state_dict(state_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f'checkpoint/subnet,{args.epochs_Adam}Adam,{args.epochs_LBFGS}LBFGS'\n",
    "if os.path.exists(file)==False:\n",
    "    os.mkdir(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{file}/loss_r_log.npy',np.array(trainer.loss_r_log))\n",
    "np.save(f'{file}/loss_b_log.npy',np.array(trainer.loss_b_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247980e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.loss_r_log = np.load(f'{file}/loss_r_log.npy')\n",
    "trainer.loss_b_log = np.load(f'{file}/loss_b_log.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    plt.rcParams.update({'font.size':16})\n",
    "    plt.plot(np.arange(len(trainer.loss_r_log)), trainer.loss_r_log, label='$ \\mathcal{L}_r$')\n",
    "    plt.plot(np.arange(len(trainer.loss_b_log)), trainer.loss_b_log, label='$ \\mathcal{L}_{ic}$')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366bdbe",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7680c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.cpu().double();\n",
    "trainer.Unet.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02dbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 51)\n",
    "y = np.linspace(0, 1, 51)\n",
    "t = np.linspace(0, 1, 11)\n",
    "U_star = np.zeros((11, 51, 51))\n",
    "# t = np.array([0,2.5,5,10])\n",
    "# U_star = np.zeros((4, 51, 51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b87061",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, YY = np.meshgrid(x, y)\n",
    "XX = torch.from_numpy(XX).double()\n",
    "YY = torch.from_numpy(YY).double()\n",
    "nt = 11\n",
    "\n",
    "U_pred_torch = torch.zeros([nt,XX.shape[0],XX.shape[1]]).double()\n",
    "\n",
    "for i in range(nt):\n",
    "    time = torch.ones([len(XX[0]), 1]) * t[i]\n",
    "    for j in range(XX.shape[1]):\n",
    "        txy = torch.hstack([time, YY[:,[j]], XX[:,[j]]])\n",
    "        u_pred = trainer.net_u(txy)\n",
    "        U_pred_torch[i][:,[j]] = u_pred.detach()\n",
    "\n",
    "XX = XX.numpy()\n",
    "YY = YY.numpy()\n",
    "U_pred = U_pred_torch.numpy()\n",
    "\n",
    "# error_u = np.linalg.norm(U_pred - U_star) / np.linalg.norm(U_star)\n",
    "# print('u Relative l2 error: {:.3e}'.format(error_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73736c-1209-49d2-8d55-c6edaa93667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(XX, YY, U_pred[0], vmin = -1, vmax =1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$t=0$')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(XX, YY, U_pred[1], vmin = -1, vmax =1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$t=1$')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(XX, YY, U_pred[2], vmin = -1, vmax =1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$t=2$')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8d148-e040-4715-b38d-b43c616e0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 51)\n",
    "y = np.linspace(0, 1, 51)\n",
    "t = np.array([0,2.5,5,10])\n",
    "U_star = np.zeros((4, 51, 51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07006c0-5f85-4f2c-9cf4-b749ae292175",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, YY = np.meshgrid(x, y)\n",
    "XX = torch.from_numpy(XX).double()\n",
    "YY = torch.from_numpy(YY).double()\n",
    "nt = 4\n",
    "\n",
    "U_pred_torch = torch.zeros([nt,XX.shape[0],XX.shape[1]]).double()\n",
    "\n",
    "for i in range(nt):\n",
    "    time = torch.ones([len(XX[0]), 1]) * t[i]\n",
    "    for j in range(XX.shape[1]):\n",
    "        txy = torch.hstack([time, YY[:,[j]], XX[:,[j]]])\n",
    "        u_pred = trainer.net_u(txy)\n",
    "        U_pred_torch[i][:,[j]] = u_pred.detach()\n",
    "\n",
    "XX = XX.numpy()\n",
    "YY = YY.numpy()\n",
    "U_pred = U_pred_torch.numpy()\n",
    "\n",
    "# error_u = np.linalg.norm(U_pred - U_star) / np.linalg.norm(U_star)\n",
    "# print('u Relative l2 error: {:.3e}'.format(error_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e740370",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.pcolor(XX, YY, U_pred[0], vmin = -1, vmax =1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$t=0$')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.pcolor(XX, YY, U_pred[1], vmin = -1, vmax =1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$t=2.5$')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.pcolor(XX, YY, U_pred[2], vmin = -1, vmax =1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$t=5$')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.pcolor(XX, YY, U_pred[3], vmin = -1, vmax =1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('$t=10$')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1fab6-28b3-4550-9ca9-860e59e8aa12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
