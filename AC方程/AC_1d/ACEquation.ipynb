{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d2238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "from pyDOE import lhs\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "plt.rcParams.update({'font.size':18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f50a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1024):\n",
    "    #     random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9dccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = (0, 1, -1, 1)\n",
    "tmin, tmax, xmin, xmax = domain\n",
    "mlp_layers = [2] + [20]*4 + [1]\n",
    "# adam_iters = 40000\n",
    "adam_iters = 40000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = r'./model'\n",
    "train_info_path = r'./'\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb215911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2) (50, 2) (50, 2) (100, 2) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, domain):\n",
    "        self.domain = domain\n",
    "        self.lb = np.array([tmin, xmin])\n",
    "        self.ub = np.array([tmax, xmax])\n",
    "\n",
    "    def train_data(self, verbose=None):\n",
    "        tmin, tmax, xmin, xmax = self.domain\n",
    "        # 内部点采样\n",
    "        t_res = np.linspace(tmin, tmax, 50)\n",
    "        x_res = np.linspace(xmin, xmax, 80)\n",
    "        X_res = self.sample_xy(t_res, x_res)\n",
    "        \n",
    "        X_bcs = np.array([]).reshape((-1, 2))\n",
    "        X_bcs_l = np.concatenate([X_bcs, self.sample_xy(np.linspace(tmin, tmax, 50), np.array([xmin]))], axis=0)\n",
    "        X_bcs_u = np.concatenate([X_bcs, self.sample_xy(np.linspace(tmin, tmax, 50), np.array([xmax]))], axis=0)\n",
    "        \n",
    "        X_ics =self.sample_xy(np.array([tmin]), np.linspace(xmin, xmax, 100))\n",
    "        u_ics = self.u_ics_sol(X_ics)\n",
    "\n",
    "        return X_res, X_bcs_l, X_bcs_u, X_ics, u_ics\n",
    "\n",
    "\n",
    "    def sample_xy(self, x, y):\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        X = np.concatenate([xx.reshape((-1, 1)), yy.reshape((-1, 1))], axis=1)\n",
    "        return X\n",
    "    \n",
    "    def u_ics_sol(self, X):\n",
    "        return X[:, [1]]**2 * np.sin(2*np.pi * X[:, [1]])\n",
    "    \n",
    "    def lhs_sample_xy(self, n=500):\n",
    "        X = (self.ub - self.lb) * lhs(2, n) + self.lb\n",
    "        return X\n",
    "    \n",
    "dataset = Dataset(domain)\n",
    "# 内部点与边界点\n",
    "X_res, X_bcs_l, X_bcs_u, X_ics, u_ics = dataset.train_data()\n",
    "print(X_res.shape,X_bcs_l.shape, X_bcs_u.shape, X_ics.shape, u_ics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7353ecb",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cafebda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (model): Sequential(\n",
       "    (fc1): Linear(in_features=2, out_features=50, bias=True)\n",
       "    (act1): Tanh()\n",
       "    (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (act2): Tanh()\n",
       "    (fc3): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (act3): Tanh()\n",
       "    (fc4): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (act4): Tanh()\n",
       "    (fc5): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (act5): Tanh()\n",
       "    (fc6)): Linear(in_features=50, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(mlp_layers) - 2):\n",
    "            self.model.add_module(f'fc{i + 1}', nn.Linear(mlp_layers[i], mlp_layers[i + 1], bias=True))\n",
    "            self.model.add_module(f'act{i + 1}', nn.Tanh())\n",
    "        self.model.add_module(f'fc{len(mlp_layers) - 1})', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "mlp = MLP(mlp_layers)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d19c76",
   "metadata": {},
   "source": [
    "## 主干网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2a0e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd43469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, backbone, mu=None, sigma=None):\n",
    "        super(PINN, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, X_res, X_bcs_l, X_bcs_u, X_ics, u_ics):\n",
    "        \n",
    "        loss_res = torch.mean(self.net_f(X_res) ** 2)\n",
    "        loss_ics = torch.mean((self.net_u(X_ics)-u_ics) ** 2)\n",
    "        loss_bcs = torch.mean((self.net_u(X_bcs_l)-self.net_u(X_bcs_u)) ** 2)\n",
    "        loss_bcs_t = torch.mean((self.net_u_x(X_bcs_l)-self.net_u_x(X_bcs_u)) ** 2)\n",
    "\n",
    "        return loss_res, loss_ics,loss_bcs,loss_bcs_t\n",
    "\n",
    "    def net_u(self, X):\n",
    "        return self.backbone(X)\n",
    "    \n",
    "    def net_u_x(self,X):\n",
    "        X.requires_grad_(True)\n",
    "        u = self.net_u(X)\n",
    "        \n",
    "        # 求梯度\n",
    "        grad_u = grad(u, X)[0]\n",
    "        u_x = grad_u[:, [1]]\n",
    "        return u_x\n",
    "\n",
    "    def net_f(self, X):\n",
    "        X.requires_grad_(True)\n",
    "        u = self.net_u(X)\n",
    "#         print(u.shape)\n",
    "        # 求梯度\n",
    "        grad_u = grad(u, X)[0]\n",
    "        u_t = grad_u[:, [0]]\n",
    "        u_x = grad_u[:, [1]]\n",
    "        u_tt = grad(u_t, X)[0][:, [0]]\n",
    "        u_xx = grad(u_x, X)[0][:, [1]]\n",
    "        \n",
    "        gamma1 = 0.0001\n",
    "        gamma2 = 4\n",
    "        \n",
    "        f = u_t - gamma1*u_xx + gamma2*u**3 - gamma2*u\n",
    "        return f  \n",
    "\n",
    "pinn = PINN(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a2bb0",
   "metadata": {},
   "source": [
    "## Resample策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b8c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_resample(dataset, net_f, device=torch.device('cuda'), n_total_points=4000, n_added_points=500):\n",
    "    # 生成待采样的点集\n",
    "    X_resam = dataset.lhs_sample_xy(n=n_total_points)\n",
    "    X_resam = torch.from_numpy(X_resam).float().to(device)\n",
    "    # 获得residule\n",
    "    f = net_f(X_resam)\n",
    "    f = f.detach().cpu().numpy()\n",
    "    # 依loss_res的值排序得到索引\n",
    "    idx = np.argsort(abs(f).flatten())[-n_added_points:]\n",
    "    return X_resam[idx].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95020ce5",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a73d1081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #    500/40000\tloss:5.74e+01, loss_r:1.24e-01, loss_i:4.18e-02 , loss_b:3.17e-02, loss_b_t:2.84e-02  Valid # loss:2.26e-01, loss_r:1.24e-01, loss_i:4.17e-02, loss_b:3.17e-02, loss_b_t:2.88e-02\n",
      "Iter #   1000/40000\tloss:4.87e+01, loss_r:1.60e-01, loss_i:3.14e-02 , loss_b:1.28e-02, loss_b_t:4.53e-02  Valid # loss:2.48e-01, loss_r:1.55e-01, loss_i:3.29e-02, loss_b:1.11e-02, loss_b_t:4.83e-02\n",
      "Iter #   1500/40000\tloss:5.28e+01, loss_r:1.83e-01, loss_i:3.41e-02 , loss_b:2.50e-03, loss_b_t:1.59e-01  Valid # loss:3.79e-01, loss_r:1.83e-01, loss_i:3.41e-02, loss_b:2.50e-03, loss_b_t:1.59e-01\n",
      "Iter #   2000/40000\tloss:5.14e+01, loss_r:1.93e-01, loss_i:3.17e-02 , loss_b:1.59e-03, loss_b_t:2.24e-01  Valid # loss:4.50e-01, loss_r:1.93e-01, loss_i:3.17e-02, loss_b:1.58e-03, loss_b_t:2.24e-01\n",
      "Iter #   2500/40000\tloss:5.52e+01, loss_r:1.93e-01, loss_i:3.58e-02 , loss_b:9.46e-04, loss_b_t:9.27e-02  Valid # loss:3.23e-01, loss_r:1.94e-01, loss_i:3.57e-02, loss_b:9.72e-04, loss_b_t:9.26e-02\n",
      "Iter #   3000/40000\tloss:5.45e+01, loss_r:2.02e-01, loss_i:3.42e-02 , loss_b:4.92e-04, loss_b_t:4.16e-02  Valid # loss:2.80e-01, loss_r:2.04e-01, loss_i:3.40e-02, loss_b:5.36e-04, loss_b_t:4.18e-02\n",
      "Iter #   3500/40000\tloss:5.77e+01, loss_r:1.97e-01, loss_i:3.79e-02 , loss_b:3.85e-04, loss_b_t:2.37e-02  Valid # loss:2.59e-01, loss_r:1.97e-01, loss_i:3.79e-02, loss_b:3.85e-04, loss_b_t:2.37e-02\n",
      "Iter #   4000/40000\tloss:5.76e+01, loss_r:1.98e-01, loss_i:3.78e-02 , loss_b:3.70e-04, loss_b_t:1.57e-02  Valid # loss:2.51e-01, loss_r:1.98e-01, loss_i:3.78e-02, loss_b:3.67e-04, loss_b_t:1.57e-02\n",
      "Iter #   4500/40000\tloss:5.98e+01, loss_r:1.90e-01, loss_i:4.07e-02 , loss_b:4.66e-04, loss_b_t:8.93e-03  Valid # loss:2.39e-01, loss_r:1.89e-01, loss_i:4.09e-02, loss_b:4.37e-04, loss_b_t:8.86e-03\n",
      "Iter #   5000/40000\tloss:5.97e+01, loss_r:1.89e-01, loss_i:4.07e-02 , loss_b:5.43e-04, loss_b_t:6.05e-03  Valid # loss:2.37e-01, loss_r:1.90e-01, loss_i:4.07e-02, loss_b:5.53e-04, loss_b_t:6.06e-03\n",
      "Iter #   5500/40000\tloss:6.12e+01, loss_r:1.83e-01, loss_i:4.28e-02 , loss_b:7.12e-04, loss_b_t:4.34e-03  Valid # loss:2.30e-01, loss_r:1.83e-01, loss_i:4.29e-02, loss_b:7.07e-04, loss_b_t:4.33e-03\n",
      "Iter #   6000/40000\tloss:6.10e+01, loss_r:1.83e-01, loss_i:4.25e-02 , loss_b:9.46e-04, loss_b_t:4.69e-03  Valid # loss:2.31e-01, loss_r:1.83e-01, loss_i:4.26e-02, loss_b:9.45e-04, loss_b_t:4.69e-03\n",
      "Iter #   6500/40000\tloss:6.16e+01, loss_r:1.77e-01, loss_i:4.37e-02 , loss_b:2.07e-03, loss_b_t:9.77e-03  Valid # loss:2.34e-01, loss_r:1.79e-01, loss_i:4.35e-02, loss_b:2.16e-03, loss_b_t:9.80e-03\n",
      "Iter #   7000/40000\tloss:5.40e+01, loss_r:1.73e-01, loss_i:3.54e-02 , loss_b:1.24e-02, loss_b_t:6.57e-02  Valid # loss:2.87e-01, loss_r:1.72e-01, loss_i:3.53e-02, loss_b:1.30e-02, loss_b_t:6.69e-02\n",
      "Iter #   7500/40000\tloss:3.87e+01, loss_r:1.27e-01, loss_i:2.51e-02 , loss_b:1.87e-03, loss_b_t:6.94e-01  Valid # loss:8.39e-01, loss_r:1.27e-01, loss_i:2.52e-02, loss_b:1.46e-03, loss_b_t:6.86e-01\n",
      "Iter #   8000/40000\tloss:3.50e+01, loss_r:1.05e-01, loss_i:2.43e-02 , loss_b:4.15e-04, loss_b_t:1.42e-01  Valid # loss:2.74e-01, loss_r:1.05e-01, loss_i:2.43e-02, loss_b:5.36e-04, loss_b_t:1.44e-01\n",
      "Iter #   8500/40000\tloss:3.38e+01, loss_r:9.19e-02, loss_i:2.45e-02 , loss_b:3.17e-04, loss_b_t:5.44e-02  Valid # loss:1.70e-01, loss_r:9.15e-02, loss_i:2.46e-02, loss_b:2.74e-04, loss_b_t:5.39e-02\n",
      "Iter #   9000/40000\tloss:3.30e+01, loss_r:8.98e-02, loss_i:2.40e-02 , loss_b:3.34e-04, loss_b_t:4.60e-02  Valid # loss:1.55e-01, loss_r:8.69e-02, loss_i:2.43e-02, loss_b:1.57e-04, loss_b_t:4.35e-02\n",
      "Iter #   9500/40000\tloss:3.42e+01, loss_r:7.27e-02, loss_i:2.62e-02 , loss_b:3.20e-03, loss_b_t:3.92e-01  Valid # loss:4.22e-01, loss_r:9.55e-02, loss_i:2.37e-02, loss_b:3.41e-03, loss_b_t:3.00e-01\n",
      "Iter #  10000/40000\tloss:3.03e+01, loss_r:1.04e-01, loss_i:1.97e-02 , loss_b:1.99e-03, loss_b_t:2.81e-02  Valid # loss:1.65e-01, loss_r:1.03e-01, loss_i:1.98e-02, loss_b:1.55e-03, loss_b_t:4.05e-02\n",
      "Iter #  10500/40000\tloss:5.54e+00, loss_r:3.90e-02, loss_i:1.12e-03 , loss_b:4.95e-03, loss_b_t:2.58e-02  Valid # loss:7.08e-02, loss_r:3.90e-02, loss_i:1.12e-03, loss_b:4.95e-03, loss_b_t:2.58e-02\n",
      "Iter #  11000/40000\tloss:5.42e+00, loss_r:2.57e-02, loss_i:1.16e-03 , loss_b:1.90e-03, loss_b_t:1.50e+00  Valid # loss:1.48e-01, loss_r:2.57e-02, loss_i:1.08e-03, loss_b:1.78e-03, loss_b_t:1.19e-01\n",
      "Iter #  11500/40000\tloss:3.44e+00, loss_r:2.02e-02, loss_i:1.17e-03 , loss_b:2.00e-03, loss_b_t:4.83e-02  Valid # loss:1.27e-01, loss_r:2.03e-02, loss_i:1.13e-03, loss_b:2.28e-03, loss_b_t:1.04e-01\n",
      "Iter #  12000/40000\tloss:3.15e+00, loss_r:1.74e-02, loss_i:1.21e-03 , loss_b:1.06e-03, loss_b_t:9.40e-02  Valid # loss:1.86e-01, loss_r:1.70e-02, loss_i:1.25e-03, loss_b:9.91e-04, loss_b_t:1.66e-01\n",
      "Iter #  12500/40000\tloss:3.30e+00, loss_r:1.68e-02, loss_i:9.10e-04 , loss_b:1.96e-03, loss_b_t:5.16e-01  Valid # loss:1.02e+00, loss_r:1.62e-02, loss_i:1.04e-03, loss_b:1.33e-03, loss_b_t:1.01e+00\n",
      "Iter #  13000/40000\tloss:3.47e+00, loss_r:1.52e-02, loss_i:8.31e-04 , loss_b:1.82e-03, loss_b_t:9.28e-01  Valid # loss:1.55e+00, loss_r:1.43e-02, loss_i:1.04e-03, loss_b:9.80e-04, loss_b_t:1.54e+00\n",
      "Iter #  13500/40000\tloss:2.93e+00, loss_r:1.30e-02, loss_i:1.07e-03 , loss_b:1.21e-03, loss_b_t:4.38e-01  Valid # loss:7.76e-01, loss_r:1.31e-02, loss_i:1.03e-03, loss_b:1.78e-03, loss_b_t:7.61e-01\n",
      "Iter #  14000/40000\tloss:3.21e+00, loss_r:1.15e-02, loss_i:1.21e-03 , loss_b:9.27e-04, loss_b_t:7.49e-01  Valid # loss:1.13e-01, loss_r:1.20e-02, loss_i:1.16e-03, loss_b:6.68e-04, loss_b_t:9.89e-02\n",
      "Iter #  14500/40000\tloss:2.54e+00, loss_r:1.11e-02, loss_i:1.13e-03 , loss_b:5.79e-04, loss_b_t:2.43e-01  Valid # loss:1.21e-01, loss_r:1.10e-02, loss_i:1.10e-03, loss_b:7.71e-04, loss_b_t:1.08e-01\n",
      "Iter #  15000/40000\tloss:2.38e+00, loss_r:1.04e-02, loss_i:1.20e-03 , loss_b:8.26e-04, loss_b_t:5.78e-02  Valid # loss:1.43e-01, loss_r:1.06e-02, loss_i:1.20e-03, loss_b:1.15e-03, loss_b_t:1.30e-01\n",
      "Iter #  15500/40000\tloss:5.42e+00, loss_r:2.32e-02, loss_i:1.46e-03 , loss_b:2.09e-03, loss_b_t:1.43e+00  Valid # loss:1.06e+00, loss_r:2.00e-02, loss_i:1.77e-03, loss_b:1.38e-03, loss_b_t:1.04e+00\n",
      "Iter #  16000/40000\tloss:1.87e+01, loss_r:1.08e-01, loss_i:6.90e-03 , loss_b:3.85e-03, loss_b_t:6.19e-01  Valid # loss:7.97e-01, loss_r:1.00e-01, loss_i:6.76e-03, loss_b:2.68e-03, loss_b_t:6.87e-01\n",
      "Iter #  16500/40000\tloss:2.41e+00, loss_r:1.04e-02, loss_i:1.30e-03 , loss_b:5.49e-04, loss_b_t:1.84e-02  Valid # loss:1.34e-01, loss_r:1.03e-02, loss_i:1.26e-03, loss_b:4.07e-04, loss_b_t:1.22e-01\n",
      "Iter #  17000/40000\tloss:3.54e+00, loss_r:1.65e-02, loss_i:1.75e-03 , loss_b:1.22e-04, loss_b_t:1.15e-01  Valid # loss:1.34e-01, loss_r:1.61e-02, loss_i:1.70e-03, loss_b:7.47e-05, loss_b_t:1.16e-01\n",
      "Iter #  17500/40000\tloss:2.83e+00, loss_r:1.06e-02, loss_i:1.61e-03 , loss_b:1.10e-04, loss_b_t:1.45e-01  Valid # loss:1.40e-01, loss_r:9.96e-03, loss_i:1.62e-03, loss_b:1.33e-04, loss_b_t:1.29e-01\n",
      "Iter #  18000/40000\tloss:2.01e+00, loss_r:6.78e-03, loss_i:1.23e-03 , loss_b:7.69e-04, loss_b_t:2.02e-02  Valid # loss:2.64e-02, loss_r:6.54e-03, loss_i:1.24e-03, loss_b:8.85e-04, loss_b_t:1.77e-02\n",
      "Iter #  18500/40000\tloss:6.71e+00, loss_r:6.82e-03, loss_i:1.40e-03 , loss_b:3.64e-04, loss_b_t:4.59e+00  Valid # loss:3.67e-01, loss_r:7.16e-03, loss_i:1.35e-03, loss_b:2.21e-04, loss_b_t:3.58e-01\n",
      "Iter #  19000/40000\tloss:1.91e+00, loss_r:6.96e-03, loss_i:1.10e-03 , loss_b:1.05e-03, loss_b_t:1.70e-02  Valid # loss:6.26e-02, loss_r:7.06e-03, loss_i:1.09e-03, loss_b:9.84e-04, loss_b_t:5.34e-02\n",
      "Iter #  19500/40000\tloss:2.94e+00, loss_r:1.34e-02, loss_i:1.55e-03 , loss_b:2.29e-04, loss_b_t:2.45e-02  Valid # loss:5.93e-02, loss_r:1.37e-02, loss_i:1.53e-03, loss_b:1.65e-04, loss_b_t:4.40e-02\n",
      "Iter #  20000/40000\tloss:2.16e+00, loss_r:7.99e-03, loss_i:1.33e-03 , loss_b:1.63e-04, loss_b_t:1.23e-02  Valid # loss:2.18e-02, loss_r:8.00e-03, loss_i:1.30e-03, loss_b:1.77e-04, loss_b_t:1.23e-02\n",
      "Iter #  20500/40000\tloss:1.83e+00, loss_r:6.85e-03, loss_i:1.08e-03 , loss_b:6.44e-04, loss_b_t:2.90e-03  Valid # loss:1.09e-02, loss_r:6.87e-03, loss_i:1.07e-03, loss_b:6.68e-04, loss_b_t:2.34e-03\n",
      "Iter #  21000/40000\tloss:2.62e+00, loss_r:9.47e-03, loss_i:1.64e-03 , loss_b:1.50e-04, loss_b_t:1.16e-02  Valid # loss:2.32e-02, loss_r:9.16e-03, loss_i:1.62e-03, loss_b:2.02e-04, loss_b_t:1.22e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #  21500/40000\tloss:2.94e+00, loss_r:9.79e-03, loss_i:1.91e-03 , loss_b:2.87e-04, loss_b_t:2.07e-02  Valid # loss:3.71e-02, loss_r:1.04e-02, loss_i:1.80e-03, loss_b:1.47e-04, loss_b_t:2.48e-02\n",
      "Iter #  22000/40000\tloss:4.54e+00, loss_r:1.77e-02, loss_i:2.63e-03 , loss_b:3.23e-04, loss_b_t:1.04e-01  Valid # loss:1.14e-01, loss_r:1.88e-02, loss_i:2.46e-03, loss_b:5.28e-04, loss_b_t:9.26e-02\n",
      "Iter #  22500/40000\tloss:2.37e+00, loss_r:8.86e-03, loss_i:1.46e-03 , loss_b:1.38e-04, loss_b_t:7.79e-03  Valid # loss:2.16e-02, loss_r:9.20e-03, loss_i:1.41e-03, loss_b:6.82e-05, loss_b_t:1.09e-02\n",
      "Iter #  23000/40000\tloss:3.03e+00, loss_r:1.06e-02, loss_i:1.89e-03 , loss_b:3.34e-05, loss_b_t:8.34e-02  Valid # loss:7.95e-02, loss_r:1.07e-02, loss_i:1.92e-03, loss_b:3.02e-05, loss_b_t:6.68e-02\n",
      "Iter #  23500/40000\tloss:3.51e+00, loss_r:1.72e-02, loss_i:1.74e-03 , loss_b:3.21e-04, loss_b_t:1.84e-02  Valid # loss:3.82e-02, loss_r:1.57e-02, loss_i:1.77e-03, loss_b:2.68e-04, loss_b_t:2.04e-02\n",
      "Iter #  24000/40000\tloss:1.74e+00, loss_r:6.46e-03, loss_i:1.03e-03 , loss_b:5.40e-04, loss_b_t:1.20e-02  Valid # loss:5.57e-02, loss_r:6.45e-03, loss_i:1.03e-03, loss_b:5.18e-04, loss_b_t:4.77e-02\n",
      "Iter #  24500/40000\tloss:2.00e+00, loss_r:7.49e-03, loss_i:1.23e-03 , loss_b:9.01e-05, loss_b_t:5.02e-03  Valid # loss:1.36e-02, loss_r:7.47e-03, loss_i:1.22e-03, loss_b:9.48e-05, loss_b_t:4.85e-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4abd4648200f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_res\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_ics\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_bcs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_bcs_t\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0moptimizer_adam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Dataset(domain)\n",
    "X_res, X_bcs_l, X_bcs_u, X_ics, u_ics = dataset.train_data()\n",
    "\n",
    "X_res = torch.from_numpy(X_res).float().to(device)\n",
    "X_bcs_l = torch.from_numpy(X_bcs_l).float().to(device)\n",
    "X_bcs_u = torch.from_numpy(X_bcs_u).float().to(device)\n",
    "X_ics = torch.from_numpy(X_ics).float().to(device)\n",
    "u_ics = torch.from_numpy(u_ics).float().to(device)\n",
    "\n",
    "mu = X_res.mean(dim=0)\n",
    "sigma = X_res.std(dim=0)  # 求样本标准差\n",
    "\n",
    "backbone = MLP(mlp_layers)  # 主干网络\n",
    "pinn = PINN(backbone, mu, sigma).to(device)\n",
    "\n",
    "optimizer_adam = optim.Adam(pinn.backbone.parameters(), lr=1e-3)\n",
    "\n",
    "lr_sche = optim.lr_scheduler.ExponentialLR(optimizer_adam, gamma=0.8)  # 指数衰减学习率\n",
    "\n",
    "logger = {\n",
    "    \"loss\": [], \n",
    "    \"loss_res\": [],\n",
    "    \"loss_ics\": [],\n",
    "    \"loss_bcs\": [],\n",
    "    \"loss_bcs_t\": [],\n",
    "    \"iter\": [],\n",
    "    \"mu\": mu,\n",
    "    \"sigma\": sigma\n",
    "}\n",
    "best_loss = 1e9\n",
    "\n",
    "# 训练\n",
    "start_time = time.time()\n",
    "for it in range(adam_iters):\n",
    "    pinn.train()\n",
    "    pinn.zero_grad()\n",
    "    \n",
    "    loss_res, loss_ics,loss_bcs,loss_bcs_t = pinn(X_res, X_bcs_l, X_bcs_u, X_ics, u_ics)\n",
    "    loss = (loss_res + loss_ics + loss_bcs + loss_bcs_t)*100\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    \n",
    "    if (it + 1) % 100 == 0:\n",
    "        # 保存loss信息\n",
    "        pinn.train(False)\n",
    "        loss_res_valid, loss_ics_valid, loss_bcs_valid, loss_bcs_t_valid = pinn(X_res, X_bcs_l, X_bcs_u, X_ics, u_ics)\n",
    "        loss_valid = loss_res_valid + loss_ics_valid + loss_bcs_valid + loss_bcs_t_valid\n",
    "        \n",
    "        logger[\"loss\"].append(loss_valid.item())\n",
    "        logger[\"loss_res\"].append(loss_res_valid.item())\n",
    "        logger[\"loss_ics\"].append(loss_ics_valid.item())\n",
    "        logger[\"loss_bcs\"].append(loss_bcs_valid.item())\n",
    "        logger[\"loss_bcs_t\"].append(loss_bcs_t_valid.item())\n",
    "        logger[\"iter\"].append(it+1)\n",
    "        \n",
    "        \n",
    "        # 保存训练loss最低的模型\n",
    "        if loss_valid.item() < best_loss:\n",
    "            model_state = {'iter': it+1, 'backbone_state': pinn.backbone.state_dict()}\n",
    "            torch.save(model_state, os.path.join(model_path, 'pinn_adam.pth'))\n",
    "            best_loss = loss_valid.item()\n",
    "        \n",
    "        if (it + 1) % 500 == 0:\n",
    "            # 保存并打印训练日志\n",
    "            info = f'Iter # {it+1:6d}/{adam_iters}\\t' + \\\n",
    "                f'loss:{loss.item():.2e}, loss_r:{loss_res.item():.2e}, loss_i:{loss_ics.item():.2e} , loss_b:{loss_bcs.item():.2e}, loss_b_t:{loss_bcs_t.item():.2e}  ' + \\\n",
    "                f'Valid # loss:{loss_valid.item():.2e}, loss_r:{loss_res_valid.item():.2e}, loss_i:{loss_ics_valid.item():.2e}, loss_b:{loss_bcs_valid.item():.2e}, loss_b_t:{loss_bcs_t_valid.item():.2e}'\n",
    "            with open(train_info_path + 'train_info.txt', 'a') as f:\n",
    "                f.write(info + '\\n')\n",
    "            print(info)\n",
    "            \n",
    "        # 衰减学习率\n",
    "        if (it + 1) % 4000 == 0:\n",
    "            lr_sche.step()\n",
    "            \n",
    "    if (it + 1) % 1000 == 0:\n",
    "        pinn.zero_grad()\n",
    "        pinn.eval()\n",
    "        # 进行重采样\n",
    "#         print(X_res.shape)\n",
    "        X_resam1 = easy_resample(dataset, pinn.net_f, device = device).float()\n",
    "        # 拼接数据\n",
    "        X_res = torch.cat([X_res, X_resam1], dim=0)\n",
    "#         print(X_res.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01659246",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./logger.npy\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = np.load(\"./logger.npy\", allow_pickle=True).item()\n",
    "k = 2\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    plt.subplot(111)\n",
    "    # plt.plot(logger[\"iter\"][::k], logger[\"loss\"][::k], label=r\"$L$\")\n",
    "    plt.plot(logger[\"iter\"][::k], logger[\"loss_res\"][::k], label=r\"$\\mathcal{L}_{r}$\", linewidth=3)\n",
    "    plt.plot(logger[\"iter\"][::k], logger[\"loss_ics\"][::k], label=r\"$\\mathcal{L}_{ics}$\", linewidth=3)\n",
    "    plt.legend()\n",
    "    plt.xticks([0, 5000, 10000, 15000, 20000])\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('loss.png', dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463cfdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pinn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-937df8ceca38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mu_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpinn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_u\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pinn' is not defined"
     ]
    }
   ],
   "source": [
    "t = np.linspace(tmin, tmax, 100)\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "tt, xx = np.meshgrid(t, x)\n",
    "X = np.concatenate([tt.reshape((-1, 1)), xx.reshape((-1, 1))], axis=1)\n",
    "print(tt.shape)\n",
    "X = torch.from_numpy(X).float().to(device)\n",
    "\n",
    "u_pred = pinn.net_u(X).detach().cpu().numpy()\n",
    "u_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d437a992",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-875de35b9bf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpcolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'jet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# plt.clim([-1., 1.])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "plt.pcolor(tt, xx, u_pred.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "plt.colorbar()\n",
    "# plt.clim([-1., 1.])\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_xticks(np.linspace(0, 1, 5))\n",
    "ax.set_yticks(np.linspace(-1, 1, 5))\n",
    "plt.title(r'Predicted $u(t,x)$')\n",
    "ax.set_aspect(1./ax.get_data_ratio())\n",
    "plt.tight_layout()\n",
    "plt.savefig('AC_pred_viridis.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29938ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
