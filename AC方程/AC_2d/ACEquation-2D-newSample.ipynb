{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d2238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "from pyDOE import lhs\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "plt.rcParams.update({'font.size':18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f50a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1024):\n",
    "    #     random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9dccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = (0, 10, 0, 1,0, 1)\n",
    "tmin, tmax, xmin, xmax, ymin, ymax = domain\n",
    "mlp_layers = [3] + [20]*4 + [1]\n",
    "# adam_iters = 40000\n",
    "adam_iters = 20000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = r'./model'\n",
    "train_info_path = r'./'\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb215911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 3) (4000, 3) (4000, 1)\n"
     ]
    }
   ],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, domain):\n",
    "        self.domain = domain\n",
    "        self.lb = np.array([tmin, xmin, ymin])\n",
    "        self.ub = np.array([tmax, xmax, ymax])\n",
    "        self.N_max = 100000\n",
    "        self.x_range = (xmin, xmax)\n",
    "        self.y_range = (ymin, ymax)\n",
    "        self.t_range = (tmin, tmax)\n",
    "\n",
    "    def train_data(self, verbose=None):\n",
    "        tmin, tmax, xmin, xmax, ymin, ymax = self.domain\n",
    "        # 内部点采样\n",
    "        t_res = np.linspace(tmin, tmax, 50)\n",
    "        x_res = np.linspace(xmin, xmax, 40)\n",
    "        y_res = np.linspace(ymin, ymax, 40)\n",
    "        X_res = self.sample_xy(x_res, y_res)\n",
    "        \n",
    "        N = X_res.shape[0]  # N=1600\n",
    "        T = t_res.shape[0]  # T=50\n",
    "        \n",
    "        XX = np.tile(X_res[:,0:1], (1,T))  # N,T\n",
    "        YY = np.tile(X_res[:,1:2], (1,T))  # N,T\n",
    "        TT = np.tile(t_res.T, (N,1))  # N,T\n",
    "        \n",
    "        x = XX.flatten()[:, None]  # NT x 1\n",
    "        y = YY.flatten()[:, None]  # NT x 1\n",
    "        t = TT.flatten()[:, None]  # NT x 1\n",
    "        \n",
    "        X_res = np.concatenate([t.reshape((-1, 1)), x.reshape((-1, 1)), y.reshape((-1, 1))], axis=1)\n",
    "# #         print(X_res.shape)\n",
    "        \n",
    "#         # 新的采样方法\n",
    "#         X = torch.Tensor(self.N_max, 1).uniform_(*self.x_range)\n",
    "#         Y = torch.Tensor(self.N_max, 1).uniform_(*self.y_range)\n",
    "#         T = torch.Tensor(self.N_max, 1).uniform_(*self.t_range)\n",
    "        \n",
    "#         idx = torch.randperm(self.N_max)\n",
    "#         idx = idx[:50000]\n",
    "#         X_res = torch.cat([T[idx], X[idx], Y[idx]], dim=1)\n",
    "\n",
    "        # 初始点采样\n",
    "        t_ics = np.ones((10,1))*tmin\n",
    "        x_ics = np.linspace(xmin, xmax, 20)\n",
    "        y_ics = np.linspace(ymin, ymax, 20)\n",
    "        X_ics = self.sample_xy(x_ics, y_ics)\n",
    "        \n",
    "        N = X_ics.shape[0]  # N=1600\n",
    "        T = t_ics.shape[0]  # T=50\n",
    "        \n",
    "        XX = np.tile(X_ics[:,0:1], (1,T))  # N,T\n",
    "        YY = np.tile(X_ics[:,1:2], (1,T))  # N,T\n",
    "        TT = np.tile(t_ics.T, (N,1))  # N,T\n",
    "        \n",
    "        x = XX.flatten()[:, None]  # NT x 1\n",
    "        y = YY.flatten()[:, None]  # NT x 1\n",
    "        t = TT.flatten()[:, None]  # NT x 1\n",
    "        \n",
    "        X_ics = np.concatenate([t.reshape((-1, 1)), x.reshape((-1, 1)), y.reshape((-1, 1))], axis=1)\n",
    "\n",
    "#         # 新的采样方法\n",
    "#         X = torch.Tensor(self.N_max, 1).uniform_(*self.x_range)\n",
    "#         Y = torch.Tensor(self.N_max, 1).uniform_(*self.y_range)\n",
    "#         T = torch.Tensor(self.N_max, 1).fill_(0)\n",
    "        \n",
    "#         idx = torch.randperm(self.N_max)\n",
    "#         idx = idx[:5000]\n",
    "#         X_ics = torch.cat([T[idx], X[idx], Y[idx]], dim=1)\n",
    "        \n",
    "        \n",
    "        # X_ics = np.concatenate([t_ics.reshape((-1,1)), x_ics.reshape((-1, 1)), y_ics.reshape((-1, 1))], axis=1)\n",
    "        u_ics = self.u_ics_sol(X_ics)\n",
    "        \n",
    "#         # 降低一维\n",
    "#         X_res = np.squeeze(X_res)\n",
    "#         X_ics = np.squeeze(X_ics)\n",
    "#         u_ics = np.squeeze(u_ics)\n",
    "\n",
    "        return X_res, X_ics, u_ics\n",
    "\n",
    "\n",
    "    def sample_xy(self, x, y):\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        X = np.concatenate([xx.reshape((-1, 1)), yy.reshape((-1, 1))], axis=1)\n",
    "        return X\n",
    "    \n",
    "#     def sample_xyz(self, t,x,y):\n",
    "#         tt, xx, yy = np.meshgrid(t,x,y)\n",
    "#         X = np.concatenate([tt[:,:,:,None],xx[:,:,:,None],yy[:,:,:,None]], axis=1)\n",
    "#         return X\n",
    "    \n",
    "    def u_ics_sol(self, X):\n",
    "        return np.tanh((0.35-np.sqrt((X[:,[1]]-0.5)**2 + (X[:,[2]]-0.5)**2))/(2*0.025))\n",
    "    \n",
    "    def lhs_sample_xy(self, n=200):\n",
    "        X = (self.ub - self.lb) * lhs(3, n) + self.lb\n",
    "        print(X.shape)\n",
    "        return X\n",
    "    \n",
    "dataset = Dataset(domain)\n",
    "# 内部点与边界点\n",
    "X_res, X_ics, u_ics = dataset.train_data()\n",
    "print(X_res.shape, X_ics.shape,u_ics.shape)\n",
    "# dataset.lhs_sample_xy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3f0b73-a500-4eb8-91a5-7231653b1afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.20408163,  0.        ,  0.        ],\n",
       "       [ 0.40816327,  0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 9.59183673,  1.        ,  1.        ],\n",
       "       [ 9.79591837,  1.        ,  1.        ],\n",
       "       [10.        ,  1.        ,  1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7353ecb",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cafebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(mlp_layers)-2):\n",
    "            layer = nn.Sequential()\n",
    "            layer.add_module(f'fc{i}', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            layer.add_module(f'act{i}', nn.Tanh())\n",
    "            self.model.add_module(f'layer{i}', layer)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'layer{len(mlp_layers)-2}', last_layer)\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "    \n",
    "    \n",
    "backbone = MLP(mlp_layers)\n",
    "# nn_lam = MLP(nn_lam_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d19c76",
   "metadata": {},
   "source": [
    "## 主干网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a0e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True, \n",
    "                               retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd43469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, backbone, mu=None, sigma=None):\n",
    "        super(PINN, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, X_res, X_ics, u_ics):\n",
    "        \n",
    "        loss_res = torch.mean(self.net_f(X_res) ** 2)\n",
    "        # print((self.net_u(X_ics)-u_ics).shape)\n",
    "        loss_ics = torch.mean((self.net_u(X_ics)-u_ics) ** 2)\n",
    "        return loss_res, loss_ics\n",
    "    \n",
    "    def net_u(self, X):\n",
    "        return self.backbone(X)\n",
    "    \n",
    "    def net_u_x(self,X):\n",
    "        X.requires_grad_(True)\n",
    "        u = self.net_u(X)\n",
    "        \n",
    "        # 求梯度\n",
    "        grad_u = grad(u, X)[0]\n",
    "        u_x = grad_u[:, [1]]\n",
    "        return u_x\n",
    "\n",
    "    def net_f(self, X):\n",
    "        X.requires_grad_(True)\n",
    "\n",
    "        u = self.net_u(X)\n",
    "#         print(u.shape)\n",
    "        # 求梯度\n",
    "        grad_u = grad(u, X)[0]\n",
    "        u_t = grad_u[:, [0]]\n",
    "        u_x = grad_u[:, [1]]\n",
    "        u_y = grad_u[:, [2]]\n",
    "        \n",
    "        # u_tt = grad(u_t, X)[0][:, [0]]\n",
    "        u_xx = grad(u_x, tx)[0][:, [1]]\n",
    "        u_yy = grad(u_y, tx)[0][:, [2]]\n",
    "        \n",
    "        eps = 0.025\n",
    "        lam = 10\n",
    "        \n",
    "        f = u_t - (eps**2 * (u_xx + u_yy) - u**3 + u) * lam\n",
    "        return f\n",
    "\n",
    "pinn = PINN(mlp_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3cc83e",
   "metadata": {},
   "source": [
    "## Resample策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb16bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_resample(dataset, net_f, device=torch.device('cuda'), n_total_points=4000, n_added_points=100):\n",
    "    # 生成待采样的点集\n",
    "    X_resam = dataset.lhs_sample_xy(n=n_total_points)\n",
    "    X_resam = torch.from_numpy(X_resam).float().to(device)\n",
    "    # 获得residule\n",
    "    f = net_f(X_resam)\n",
    "    f = f.detach().cpu().numpy()\n",
    "    # 依loss_res的值排序得到索引\n",
    "    idx = np.argsort(abs(f).flatten())[-n_added_points:]\n",
    "    return X_resam[idx].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95020ce5",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a73d1081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #    500/20000\tloss:9.89e+00, loss_r:1.18e+00, loss_i:8.71e-02  Valid # loss:1.27e+00, loss_r:1.18e+00, loss_i:8.69e-02\n",
      "Iter #   1000/20000\tloss:5.62e+00, loss_r:1.12e+00, loss_i:4.50e-02  Valid # loss:1.17e+00, loss_r:1.12e+00, loss_i:4.49e-02\n",
      "(4000, 3)\n",
      "Iter #   1500/20000\tloss:3.42e+00, loss_r:1.02e+00, loss_i:2.40e-02  Valid # loss:1.04e+00, loss_r:1.02e+00, loss_i:2.40e-02\n",
      "Iter #   2000/20000\tloss:1.97e+00, loss_r:9.39e-01, loss_i:1.03e-02  Valid # loss:9.49e-01, loss_r:9.38e-01, loss_i:1.03e-02\n",
      "(4000, 3)\n",
      "Iter #   2500/20000\tloss:1.34e+00, loss_r:8.89e-01, loss_i:4.51e-03  Valid # loss:8.93e-01, loss_r:8.89e-01, loss_i:4.50e-03\n",
      "Iter #   3000/20000\tloss:1.03e+00, loss_r:8.00e-01, loss_i:2.28e-03  Valid # loss:8.02e-01, loss_r:8.00e-01, loss_i:2.28e-03\n",
      "(4000, 3)\n",
      "Iter #   3500/20000\tloss:8.66e-01, loss_r:7.41e-01, loss_i:1.25e-03  Valid # loss:7.42e-01, loss_r:7.41e-01, loss_i:1.25e-03\n",
      "Iter #   4000/20000\tloss:7.05e-01, loss_r:6.28e-01, loss_i:7.73e-04  Valid # loss:6.28e-01, loss_r:6.27e-01, loss_i:7.73e-04\n",
      "(4000, 3)\n",
      "Iter #   4500/20000\tloss:6.18e-01, loss_r:5.60e-01, loss_i:5.80e-04  Valid # loss:5.61e-01, loss_r:5.60e-01, loss_i:5.80e-04\n",
      "Iter #   5000/20000\tloss:5.29e-01, loss_r:4.84e-01, loss_i:4.47e-04  Valid # loss:4.85e-01, loss_r:4.84e-01, loss_i:4.47e-04\n",
      "(4000, 3)\n",
      "Iter #   5500/20000\tloss:5.24e-01, loss_r:4.91e-01, loss_i:3.25e-04  Valid # loss:4.91e-01, loss_r:4.91e-01, loss_i:3.25e-04\n",
      "Iter #   6000/20000\tloss:4.73e-01, loss_r:4.50e-01, loss_i:2.29e-04  Valid # loss:4.50e-01, loss_r:4.50e-01, loss_i:2.29e-04\n",
      "(4000, 3)\n",
      "Iter #   6500/20000\tloss:4.84e-01, loss_r:4.68e-01, loss_i:1.65e-04  Valid # loss:4.68e-01, loss_r:4.68e-01, loss_i:1.65e-04\n",
      "Iter #   7000/20000\tloss:4.19e-01, loss_r:4.06e-01, loss_i:1.33e-04  Valid # loss:4.06e-01, loss_r:4.06e-01, loss_i:1.33e-04\n",
      "(4000, 3)\n",
      "Iter #   7500/20000\tloss:3.85e-01, loss_r:3.73e-01, loss_i:1.24e-04  Valid # loss:3.72e-01, loss_r:3.72e-01, loss_i:1.24e-04\n",
      "Iter #   8000/20000\tloss:3.20e-01, loss_r:3.08e-01, loss_i:1.20e-04  Valid # loss:3.08e-01, loss_r:3.08e-01, loss_i:1.20e-04\n",
      "(4000, 3)\n",
      "Iter #   8500/20000\tloss:3.09e-01, loss_r:2.97e-01, loss_i:1.15e-04  Valid # loss:2.97e-01, loss_r:2.97e-01, loss_i:1.15e-04\n",
      "Iter #   9000/20000\tloss:2.67e-01, loss_r:2.58e-01, loss_i:9.62e-05  Valid # loss:2.58e-01, loss_r:2.58e-01, loss_i:9.63e-05\n",
      "(4000, 3)\n",
      "Iter #   9500/20000\tloss:3.06e-01, loss_r:2.95e-01, loss_i:1.11e-04  Valid # loss:2.95e-01, loss_r:2.95e-01, loss_i:1.13e-04\n",
      "Iter #  10000/20000\tloss:2.85e-01, loss_r:2.78e-01, loss_i:6.75e-05  Valid # loss:2.78e-01, loss_r:2.78e-01, loss_i:6.75e-05\n",
      "(4000, 3)\n",
      "Iter #  10500/20000\tloss:3.21e-01, loss_r:3.14e-01, loss_i:6.65e-05  Valid # loss:3.15e-01, loss_r:3.15e-01, loss_i:6.09e-05\n",
      "Iter #  11000/20000\tloss:3.09e-01, loss_r:3.03e-01, loss_i:5.70e-05  Valid # loss:3.03e-01, loss_r:3.03e-01, loss_i:5.70e-05\n",
      "(4000, 3)\n",
      "Iter #  11500/20000\tloss:3.46e-01, loss_r:3.40e-01, loss_i:5.49e-05  Valid # loss:3.40e-01, loss_r:3.40e-01, loss_i:5.57e-05\n",
      "Iter #  12000/20000\tloss:3.35e-01, loss_r:3.30e-01, loss_i:5.16e-05  Valid # loss:3.30e-01, loss_r:3.30e-01, loss_i:5.16e-05\n",
      "(4000, 3)\n",
      "Iter #  12500/20000\tloss:3.78e-01, loss_r:3.73e-01, loss_i:4.68e-05  Valid # loss:3.73e-01, loss_r:3.73e-01, loss_i:4.67e-05\n",
      "Iter #  13000/20000\tloss:3.65e-01, loss_r:3.59e-01, loss_i:5.93e-05  Valid # loss:3.60e-01, loss_r:3.60e-01, loss_i:5.70e-05\n",
      "(4000, 3)\n",
      "Iter #  13500/20000\tloss:3.97e-01, loss_r:3.93e-01, loss_i:3.95e-05  Valid # loss:3.93e-01, loss_r:3.93e-01, loss_i:3.95e-05\n",
      "Iter #  14000/20000\tloss:3.82e-01, loss_r:3.78e-01, loss_i:3.93e-05  Valid # loss:3.78e-01, loss_r:3.78e-01, loss_i:3.93e-05\n",
      "(4000, 3)\n",
      "Iter #  14500/20000\tloss:4.04e-01, loss_r:3.99e-01, loss_i:4.76e-05  Valid # loss:3.99e-01, loss_r:3.99e-01, loss_i:4.76e-05\n",
      "Iter #  15000/20000\tloss:3.79e-01, loss_r:3.74e-01, loss_i:5.28e-05  Valid # loss:3.74e-01, loss_r:3.74e-01, loss_i:5.26e-05\n",
      "(4000, 3)\n",
      "Iter #  15500/20000\tloss:4.10e-01, loss_r:4.05e-01, loss_i:5.01e-05  Valid # loss:4.05e-01, loss_r:4.05e-01, loss_i:5.15e-05\n",
      "Iter #  16000/20000\tloss:3.83e-01, loss_r:3.78e-01, loss_i:4.98e-05  Valid # loss:3.78e-01, loss_r:3.78e-01, loss_i:4.98e-05\n",
      "(4000, 3)\n",
      "Iter #  16500/20000\tloss:4.04e-01, loss_r:3.99e-01, loss_i:4.61e-05  Valid # loss:3.99e-01, loss_r:3.99e-01, loss_i:4.63e-05\n",
      "Iter #  17000/20000\tloss:3.73e-01, loss_r:3.69e-01, loss_i:4.25e-05  Valid # loss:3.69e-01, loss_r:3.69e-01, loss_i:4.25e-05\n",
      "(4000, 3)\n",
      "Iter #  17500/20000\tloss:3.94e-01, loss_r:3.89e-01, loss_i:4.90e-05  Valid # loss:3.89e-01, loss_r:3.89e-01, loss_i:5.49e-05\n",
      "Iter #  18000/20000\tloss:3.59e-01, loss_r:3.53e-01, loss_i:5.83e-05  Valid # loss:3.53e-01, loss_r:3.53e-01, loss_i:4.91e-05\n",
      "(4000, 3)\n",
      "Iter #  18500/20000\tloss:3.30e-01, loss_r:3.24e-01, loss_i:6.40e-05  Valid # loss:3.24e-01, loss_r:3.24e-01, loss_i:6.40e-05\n",
      "Iter #  19000/20000\tloss:2.87e-01, loss_r:2.82e-01, loss_i:5.03e-05  Valid # loss:2.82e-01, loss_r:2.82e-01, loss_i:5.03e-05\n",
      "(4000, 3)\n",
      "Iter #  19500/20000\tloss:3.02e-01, loss_r:2.98e-01, loss_i:4.49e-05  Valid # loss:2.98e-01, loss_r:2.98e-01, loss_i:4.49e-05\n",
      "Iter #  20000/20000\tloss:2.79e-01, loss_r:2.74e-01, loss_i:4.41e-05  Valid # loss:2.74e-01, loss_r:2.74e-01, loss_i:4.39e-05\n",
      "(4000, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(domain)\n",
    "X_res, X_ics, u_ics = dataset.train_data()\n",
    "\n",
    "X_res = torch.from_numpy(X_res).float().to(device)\n",
    "# X_res = X_res.float().to(device)\n",
    "# X_ics = X_ics.float().to(device)\n",
    "# u_ics = u_ics.float().to(device)\n",
    "X_ics = torch.from_numpy(X_ics).float().to(device)\n",
    "u_ics = torch.from_numpy(u_ics).float().to(device)\n",
    "\n",
    "mu = X_res.mean(dim=0)\n",
    "sigma = X_res.std(dim=0)  # 求样本标准差\n",
    "\n",
    "backbone = MLP(mlp_layers)  # 主干网络\n",
    "pinn = PINN(backbone, mu, sigma).to(device)\n",
    "\n",
    "optimizer_adam = optim.Adam(pinn.backbone.parameters(), lr=1e-3)\n",
    "\n",
    "lr_sche = optim.lr_scheduler.ExponentialLR(optimizer_adam, gamma=0.8)  # 指数衰减学习率\n",
    "\n",
    "logger = {\n",
    "    \"loss\": [], \n",
    "    \"loss_res\": [],\n",
    "    \"loss_ics\": [],\n",
    "    \"loss_bcs\": [],\n",
    "    \"loss_bcs_t\": [],\n",
    "    \"iter\": [],\n",
    "    \"mu\": mu,\n",
    "    \"sigma\": sigma\n",
    "}\n",
    "best_loss = 1e9\n",
    "\n",
    "# 训练\n",
    "start_time = time.time()\n",
    "for it in range(adam_iters):\n",
    "    pinn.train()\n",
    "    pinn.zero_grad()\n",
    "    \n",
    "    loss_res, loss_ics = pinn(X_res, X_ics, u_ics)\n",
    "    loss = loss_res + loss_ics*100\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    \n",
    "    if (it + 1) % 100 == 0:\n",
    "        # 保存loss信息\n",
    "        pinn.train(False)\n",
    "        loss_res_valid, loss_ics_valid = pinn(X_res, X_ics, u_ics)\n",
    "        loss_valid = loss_res_valid + loss_ics_valid \n",
    "        \n",
    "        logger[\"loss\"].append(loss_valid.item())\n",
    "        logger[\"loss_res\"].append(loss_res_valid.item())\n",
    "        logger[\"loss_ics\"].append(loss_ics_valid.item())\n",
    "        logger[\"iter\"].append(it+1)\n",
    "        \n",
    "        \n",
    "        # 保存训练loss最低的模型\n",
    "        if loss_valid.item() < best_loss:\n",
    "            model_state = {'iter': it+1, 'backbone_state': pinn.backbone.state_dict()}\n",
    "            torch.save(model_state, os.path.join(model_path, 'pinn_adam.pth'))\n",
    "            best_loss = loss_valid.item()\n",
    "        \n",
    "        if (it + 1) % 500 == 0:\n",
    "            # 保存并打印训练日志\n",
    "            info = f'Iter # {it+1:6d}/{adam_iters}\\t' + \\\n",
    "                f'loss:{loss.item():.2e}, loss_r:{loss_res.item():.2e}, loss_i:{loss_ics.item():.2e}  ' + \\\n",
    "                f'Valid # loss:{loss_valid.item():.2e}, loss_r:{loss_res_valid.item():.2e}, loss_i:{loss_ics_valid.item():.2e}'\n",
    "            with open(train_info_path + 'train_info.txt', 'a') as f:\n",
    "                f.write(info + '\\n')\n",
    "            print(info)\n",
    "            \n",
    "        # 衰减学习率\n",
    "        if (it + 1) % 4000 == 0:\n",
    "            lr_sche.step()\n",
    "            \n",
    "    if (it + 1) % 1000 == 0:\n",
    "        pinn.zero_grad()\n",
    "        pinn.eval()\n",
    "        # 进行重采样\n",
    "#         print(X_res.shape)\n",
    "        X_resam1 = easy_resample(dataset, pinn.net_f, device = device).float()\n",
    "        # 拼接数据\n",
    "        X_res = torch.cat([X_res, X_resam1], dim=0)\n",
    "#         print(X_res.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab21abb",
   "metadata": {},
   "source": [
    "## LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2493afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #  100\ttime:2.9e+04\tloss:2.57e-01, loss_res:2.33e-01, loss_ics:2.38e-02\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'best_loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1f266779f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mit\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0moptimizer_lbfgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mmodel_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpinn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    424\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0m\u001b[0;32m    427\u001b[0m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0;32m    428\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[1;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mf_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[1;34m(x, t, d)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m                         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[1;34m(self, closure, x, t, d)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-1f266779f1ae>\u001b[0m in \u001b[0;36mclosure\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# 保存训练loss最低的模型 -------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mloss_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mmodel_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'backbone_dict'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpinn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'pinn_lbfgs.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'best_loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "optimizer_lbfgs = optim.LBFGS(pinn.parameters(), max_iter=50000, history_size=50000,  # 调大history_size等价于BFGS\n",
    "                                      tolerance_grad=1.e-6, tolerance_change=1.e-10, line_search_fn=\"strong_wolfe\")\n",
    "it = 0  # BFGS迭代次数\n",
    "with open(train_info_path + 'train_info.txt', 'a') as f:\n",
    "    f.write('Training by LBFGS:\\n')\n",
    "\n",
    "def closure():\n",
    "    global it\n",
    "    pinn.zero_grad()\n",
    "    loss_res, loss_ics = pinn(X_res, X_ics, u_ics)\n",
    "    loss = loss_res + loss_ics\n",
    "    loss.backward()\n",
    "    # logger[\"loss\"].append(loss.item())\n",
    "    # logger[\"loss_res\"].append(loss_res.item())\n",
    "    # logger[\"loss_ics\"].append(loss_ics.item())\n",
    "    # logger[\"iter\"].append(it+1)\n",
    "    if (it + 1) % 100 == 0:\n",
    "        info = f'Iter # {it+1:4d}\\ttime:{time.time()-start_time:.1e}\\t' + f'loss:{loss.item():.2e}, loss_res:{loss_res.item():.2e}, loss_ics:{loss_ics.item():.2e}'\n",
    "        with open(train_info_path + 'train_info.txt', 'a') as f:\n",
    "            f.write(info + '\\n')\n",
    "        print(info)\n",
    "        # 保存训练loss最低的模型 -------\n",
    "        if loss_valid.item() < best_loss:  \n",
    "            model_state = {'backbone_dict': pinn.backbone.state_dict()}\n",
    "            torch.save(model_state, os.path.join(model_path, f'pinn_lbfgs.pth'))\n",
    "            best_loss = loss.item()\n",
    "    it = it + 1\n",
    "    return loss\n",
    "optimizer_lbfgs.step(closure)\n",
    "\n",
    "model_state = {'state_dict': pinn.state_dict()}\n",
    "backbone_path = os.path.join(model_path, f'pinn_lbfgs.pth')\n",
    "torch.save(model_state, backbone_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e3288-d365-4bbd-b361-7a713caa3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01659246",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./logger.npy\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = np.load(\"./logger.npy\", allow_pickle=True).item()\n",
    "k = 2\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    plt.subplot(111)\n",
    "    # plt.plot(logger[\"iter\"][::k], logger[\"loss\"][::k], label=r\"$L$\")\n",
    "    plt.plot(logger[\"iter\"][::k], logger[\"loss_res\"][::k], label=r\"$\\mathcal{L}_{r}$\", linewidth=3)\n",
    "    plt.plot(logger[\"iter\"][::k], logger[\"loss_ics\"][::k], label=r\"$\\mathcal{L}_{ics}$\", linewidth=3)\n",
    "    plt.legend()\n",
    "    plt.xticks([0, 5000, 10000, 15000, 20000])\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('loss.png', dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 5))\n",
    "\n",
    "t_res = np.linspace(tmin, tmax, 101)\n",
    "x_res = np.linspace(xmin, xmax, 100)\n",
    "y_res = np.linspace(ymin, ymax, 100)\n",
    "xx, yy = np.meshgrid(x_res,y_res)\n",
    "X_res = dataset.sample_xy(x_res, y_res)\n",
    "\n",
    "\n",
    "N = X_res.shape[0]  # N=1600\n",
    "T = t_res.shape[0]  # T=50\n",
    "\n",
    "XX = np.tile(X_res[:,0:1], (1,T))  # N,T\n",
    "YY = np.tile(X_res[:,1:2], (1,T))  # N,T\n",
    "TT = np.tile(t_res.T, (N,1))  # N,T\n",
    "\n",
    "snap1 = np.array([0])\n",
    "snap2 = np.array([25])\n",
    "snap3 = np.array([50])\n",
    "snap4 = np.array([100])\n",
    "\n",
    "x_star = X_res[:,0:1]\n",
    "y_star = X_res[:,1:2]\n",
    "\n",
    "t_1 = TT[:,snap1]\n",
    "t_2 = TT[:,snap2]\n",
    "t_3 = TT[:,snap3]\n",
    "t_4 = TT[:,snap4]\n",
    "\n",
    "\n",
    "x_star = torch.tensor(x_star)\n",
    "y_star = torch.tensor(y_star)\n",
    "t_1 = torch.tensor(t_1)\n",
    "t_2 = torch.tensor(t_2)\n",
    "t_3 = torch.tensor(t_3)\n",
    "t_4 = torch.tensor(t_4)\n",
    "\n",
    "X_1 = torch.cat([t_1, x_star,y_star], dim=1)\n",
    "X_2 = torch.cat([t_2, x_star,y_star], dim=1)\n",
    "X_3 = torch.cat([t_3, x_star,y_star], dim=1)\n",
    "X_4 = torch.cat([t_4, x_star,y_star], dim=1)\n",
    "X_1 = X_1.float().to(device)\n",
    "X_2 = X_2.float().to(device)\n",
    "X_3 = X_3.float().to(device)\n",
    "X_4 = X_4.float().to(device)\n",
    "print(X_1)\n",
    "print(X_2)\n",
    "\n",
    "u_pred_1 = pinn.net_u(X_1).detach().cpu().numpy()\n",
    "u_pred_2 = pinn.net_u(X_2).detach().cpu().numpy()\n",
    "u_pred_3 = pinn.net_u(X_3).detach().cpu().numpy()\n",
    "u_pred_4 = pinn.net_u(X_4).detach().cpu().numpy()\n",
    "\n",
    "print(u_pred_1.max())\n",
    "print(u_pred_2.max())\n",
    "# print(u_pred_1.shape)\n",
    "\n",
    "axes = fig.subplots(1, 4)\n",
    "plt.subplot(1,4,1)\n",
    "plt.pcolor(xx, yy, u_pred_1.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.pcolor(xx, yy, u_pred_2.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.pcolor(xx, yy, u_pred_3.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.pcolor(xx, yy, u_pred_4.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c3269-7155-45d1-8d25-c61a379641c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = np.array([2.5])\n",
    "x = np.linspace(0, 1, 101)\n",
    "y = np.linspace(0, 1, 101)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "X_1 = np.concatenate([t_1*np.ones_like(xx.reshape(-1, 1)), xx.reshape(-1, 1), yy.reshape(-1, 1)], axis=1)\n",
    "\n",
    "X_1 = torch.from_numpy(X_1).float().to(device)\n",
    "\n",
    "u_pred_1 = pinn.net_u(X_1).detach().cpu().numpy()\n",
    "\n",
    "print(u_pred_1.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b6445-4361-4ee8-ba02-98a0f7f03b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 5))\n",
    "ax = plt.subplot(1, 4, 1)\n",
    "plt.pcolor(xx, yy, u_pred_1.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "plt.colorbar()\n",
    "# plt.clim([-1., 1.])\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xticks(np.linspace(0, 1, 5))\n",
    "ax.set_yticks(np.linspace(0, 1, 5))\n",
    "plt.title(r'Predicted $u(t,x)$')\n",
    "ax.set_aspect(1./ax.get_data_ratio())\n",
    "\n",
    "# ax = plt.subplot(1, 4, 2)\n",
    "# plt.pcolor(xx, yy, u_pred_2.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "# plt.colorbar()\n",
    "# # plt.clim([-1., 1.])\n",
    "# plt.xlabel('$x$')\n",
    "# plt.ylabel('$y$')\n",
    "# ax.set_xlim([0, 1])\n",
    "# ax.set_ylim([0, 1])\n",
    "# ax.set_xticks(np.linspace(0, 1, 5))\n",
    "# ax.set_yticks(np.linspace(0, 1, 5))\n",
    "# plt.title(r'Predicted $u(t,x)$')\n",
    "# ax.set_aspect(1./ax.get_data_ratio())\n",
    "\n",
    "# ax = plt.subplot(1, 4, 3)\n",
    "# plt.pcolor(xx, yy, u_pred_3.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "# plt.colorbar()\n",
    "# # plt.clim([-1., 1.])\n",
    "# plt.xlabel('$x$')\n",
    "# plt.ylabel('$y$')\n",
    "# ax.set_xlim([0, 1])\n",
    "# ax.set_ylim([0, 1])\n",
    "# ax.set_xticks(np.linspace(0, 1, 5))\n",
    "# ax.set_yticks(np.linspace(0, 1, 5))\n",
    "# plt.title(r'Predicted $u(t,x)$')\n",
    "# ax.set_aspect(1./ax.get_data_ratio())\n",
    "\n",
    "# ax = plt.subplot(1, 4, 4)\n",
    "# plt.pcolor(xx, yy, u_pred_4.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "# plt.colorbar()\n",
    "# # plt.clim([-1., 1.])\n",
    "# plt.xlabel('$x$')\n",
    "# plt.ylabel('$y$')\n",
    "# ax.set_xlim([0, 1])\n",
    "# ax.set_ylim([0, 1])\n",
    "# ax.set_xticks(np.linspace(0, 1, 5))\n",
    "# ax.set_yticks(np.linspace(0, 1, 5))\n",
    "# plt.title(r'Predicted $u(t,x)$')\n",
    "# ax.set_aspect(1./ax.get_data_ratio())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('AC_pred.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4ccb2-1de6-416d-a3ad-ca6a99a78c2e",
   "metadata": {},
   "source": [
    "# 初始条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_res = np.linspace(tmin, tmax, 100)\n",
    "x_res = np.linspace(xmin, xmax, 100)\n",
    "y_res = np.linspace(ymin, ymax, 100)\n",
    "xx, yy = np.meshgrid(x_res,y_res)\n",
    "X_res = dataset.sample_xy(x_res, y_res)\n",
    "\n",
    "\n",
    "N = X_res.shape[0]  # N=1600\n",
    "T = t_res.shape[0]  # T=50\n",
    "\n",
    "XX = np.tile(X_res[:,0:1], (1,T))  # N,T\n",
    "YY = np.tile(X_res[:,1:2], (1,T))  # N,T\n",
    "TT = np.tile(t_res.T, (N,1))  # N,T\n",
    "\n",
    "snap = np.array([0])\n",
    "\n",
    "x_star = X_res[:,0:1]\n",
    "y_star = X_res[:,1:2]\n",
    "t_star = TT[:,snap]\n",
    "\n",
    "x_star = torch.tensor(x_star)\n",
    "y_star = torch.tensor(y_star)\n",
    "t_star = torch.tensor(t_star)\n",
    "\n",
    "X = torch.cat([t_star, x_star,y_star], dim=1)\n",
    "X = X.float().to(device).cpu().numpy()\n",
    "\n",
    "u0 = dataset.u_ics_sol(X)\n",
    "print(u0.shape)\n",
    "print()\n",
    "\n",
    "plt.pcolor(xx, yy, u0.reshape(xx.shape), cmap='jet', vmin = -1, vmax =1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2515e-e1bd-4801-849d-f07b5ab43b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
