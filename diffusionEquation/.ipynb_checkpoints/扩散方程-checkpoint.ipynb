{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35519d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4ddd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import pickle\n",
    "import scipy.io\n",
    "import random\n",
    "from math import pi\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a9079",
   "metadata": {},
   "source": [
    "## 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea4f5c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Namespace' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0f21eee6fddf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'best_model.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Namespace' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "class Options_poisson(object):\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--no_cuda', action='store_true', default=False, help='disable CUDA or not')\n",
    "        parser.add_argument('--dim_hidden', type=int, default=20, help='neurons in hidden layers') #20\n",
    "        parser.add_argument('--hidden_layers', type=int, default=4, help='number of hidden layers') #4\n",
    "        parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
    "        parser.add_argument('--epochs_Adam', type=int, default=10000, help='epochs for Adam optimizer')\n",
    "        parser.add_argument('--epochs_LBFGS', type=int, default=0, help='epochs for LBFGS optimizer')\n",
    "        parser.add_argument('--newton_iter', type=int, default=100, help='newton_iter for LBFGS optimizer')\n",
    "        parser.add_argument('--step_size', type=int, default=1000, help='step size in lr_scheduler for Adam optimizer')\n",
    "        parser.add_argument('--gamma', type=float, default=0.7, help='gamma in lr_scheduler for Adam optimizer')\n",
    "        \n",
    "        self.parser = parser\n",
    "\n",
    "    def parse(self):\n",
    "        arg = self.parser.parse_args(args=[])\n",
    "        arg.cuda = not arg.no_cuda and torch.cuda.is_available()\n",
    "        arg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        return arg\n",
    "\n",
    "args = Options_poisson().parse()\n",
    "print(args.hidden_layers)\n",
    "\n",
    "def save_model(state, is_best=None, save_dir=None):\n",
    "    last_model = os.path.join(save_dir, 'last_model.pth')\n",
    "    torch.save(state, last_model)\n",
    "    if is_best:\n",
    "        best_model = os.path.join(save_dir, 'best_model.pth')\n",
    "        shutil.copyfile(last_model, best_model)\n",
    "args[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80ab273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\" compute the derivative of outputs associated with inputs\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "    outputs: (N, 1) tensor\n",
    "    inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True)\n",
    "\n",
    "def activation(name):\n",
    "    if name in ['tanh', 'Tanh']:\n",
    "        return nn.Tanh()\n",
    "    elif name in ['relu', 'ReLU']:\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif name in ['leaky_relu', 'LeakyReLU']:\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif name in ['sigmoid', 'Sigmoid']:\n",
    "        return nn.Sigmoid()\n",
    "    elif name in ['softplus', 'Softplus']:\n",
    "        return nn.Softplus()\n",
    "    else:\n",
    "        raise ValueError(f'unknown activation function: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb4643",
   "metadata": {},
   "source": [
    "## 网络模型(DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5646b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modified_MLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden, hidden_layers,\n",
    "                 act_name='tanh', init_name='xavier_normal'):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        encoder_U = nn.Sequential()\n",
    "        encoder_U.add_module('fc', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        encoder_U.add_module('act', activation(act_name))\n",
    "        self.encoder_U = encoder_U\n",
    "\n",
    "        encoder_V = nn.Sequential()\n",
    "        encoder_V.add_module('fc', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        encoder_V.add_module('act', activation(act_name))\n",
    "        self.encoder_V = encoder_V\n",
    "\n",
    "        model = nn.Sequential()\n",
    "        model.add_module('fc0', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        model.add_module('act0', activation(act_name))\n",
    "        for i in range(1, hidden_layers):\n",
    "            model.add_module(f'fc{i}', nn.Linear(dim_hidden, dim_hidden, bias=True))\n",
    "            model.add_module(f'act{i}', activation(act_name))\n",
    "        model.add_module(f'fc{hidden_layers}', nn.Linear(dim_hidden, dim_out, bias=True))\n",
    "\n",
    "        self.model = model\n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "\n",
    "        self.size = self.model_size()\n",
    "\n",
    "    def init_weight(self, name):\n",
    "        \"\"\"初始化网络参数\"\"\"\n",
    "\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"模型的正向传播\"\"\"\n",
    "        U = self.encoder_U(x)\n",
    "        V = self.encoder_V(x)\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = self.model[2 * i](x)      # 调用线性层\n",
    "            x = self.model[2 * i + 1](x)  # 调用激活层\n",
    "            x = (1 - x) * U + x * V       # 特征融合\n",
    "        x = self.model[-1](x)             # 调用最后一个线性层得到输出\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        \"\"\"模型大小\"\"\"\n",
    "        n_params = 0\n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b220c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    \"\"\"Deep Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden, hidden_layers,\n",
    "                 act_name='tanh', init_name=None):\n",
    "        super().__init__()\n",
    "        model = nn.Sequential()\n",
    "\n",
    "        model.add_module('fc0', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        model.add_module('act0', activation(act_name))\n",
    "        for i in range(1, hidden_layers):\n",
    "            model.add_module(f'fc{i}', nn.Linear(dim_hidden, dim_hidden, bias=True))\n",
    "            model.add_module(f'act{i}', activation(act_name))\n",
    "        model.add_module(f'fc{hidden_layers}', nn.Linear(dim_hidden, dim_out, bias=True))\n",
    "\n",
    "        self.model = model\n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "\n",
    "    def init_weight(self, name):\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def forward_test(self, x):\n",
    "        print(f\"{'input':<20}{str(x.shape):<40}\")\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            print(f\"{name:<20}{str(x.shape):<40}\")\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        n_params = 0\n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89a4786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (model): Sequential(\n",
       "    (fc0): Linear(in_features=1, out_features=100, bias=True)\n",
       "    (act0): Tanh()\n",
       "    (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (act1): Tanh()\n",
       "    (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNN(1,1,100,2)\n",
    "# model.cuda()\n",
    "args.model=model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d9ab3",
   "metadata": {},
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fed8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainset():\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.data()\n",
    "    \n",
    "    def u(self, x):\n",
    "        Pe = 9.34\n",
    "        L = 4\n",
    "        C_F = 0.5\n",
    "        alpha = np.sqrt(1 + 4/Pe*0.5*6)\n",
    "        \n",
    "        numerator = 2*(1+alpha)*np.exp(Pe/2*(1+alpha))*np.exp(Pe/2*(1-alpha)*(x/L)) - 2*(1-alpha)*np.exp(Pe/2*(1-alpha)*Pe/2*(1+alpha)*(x/L))\n",
    "        denominator = (1+alpha)**2 * np.exp(Pe/2*(1+alpha)) - (1-alpha)**2 * np.exp(Pe/2*(1-alpha))\n",
    "        sol = numerator / denominator * C_F\n",
    "        return sol\n",
    "    \n",
    "    \n",
    "    def data(self):\n",
    "        Nsd = self.args[0]\n",
    "        \n",
    "        x_lb = np.array([0.0])\n",
    "        x_ub = np.array([4.0])\n",
    "        x = (x_ub-x_lb) * lhs(1, Nsd) + x_lb\n",
    "        \n",
    "        x_0 = np.array([[0.0]])\n",
    "        x_L = np.array([[4.0]])\n",
    "        \n",
    "        x = torch.from_numpy(x).float()\n",
    "        x_0 = torch.from_numpy(x_0).float()\n",
    "        x_L = torch.from_numpy(x_L).float()\n",
    "        \n",
    "        return x, x_0, x_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbf23d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Trainset at 0x2056da2e220>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = Trainset(500)\n",
    "args.trainset = trainset\n",
    "x, x_0, x_L = trainset()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccb439",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_poisson(object):\n",
    "    def __init__(self, args):\n",
    "        self.model = args.model\n",
    "        self.lr = args.lr\n",
    "        self.gamma = args.gamma\n",
    "\n",
    "        self.newton_iter = args.newton_iter\n",
    "        self.step_size = args.step_size\n",
    "        \n",
    "        self.model_name = self.model.__class__.__name__\n",
    "        self.model_path = self._model_path()\n",
    "        \n",
    "        self.epochs_Adam = args.epochs_Adam\n",
    "        self.epochs_LBFGS = args.epochs_LBFGS\n",
    "        self.optimizer_Adam = optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999))\n",
    "        self.optimizer_LBFGS = optim.LBFGS(self.model.parameters(),\n",
    "                                            lr=0.1,\n",
    "                                            max_iter=self.newton_iter,\n",
    "                                            tolerance_grad=1.e-5,\n",
    "                                            tolerance_change=1.e-9)\n",
    "        self.scheduler = lr_scheduler.ExponentialLR(self.optimizer_Adam, gamma=self.gamma, verbose=True)\n",
    "                \n",
    "        # data\n",
    "        self.x, self.x_0, self.x_L = args.trainset()\n",
    " \n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.loss_r_log = []\n",
    "        self.loss_0_log = []\n",
    "        self.loss_L_log = []\n",
    "        self.epoch_log = []\n",
    "        self.error_log = []\n",
    "        \n",
    "        \n",
    "    def _model_path(self):\n",
    "        \"\"\"Path to save the model\"\"\"\n",
    "        if not os.path.exists('checkpoints'):\n",
    "            os.mkdir('checkpoints')\n",
    "        \n",
    "        path = os.path.join('checkpoints', self.model_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    \n",
    "    def net_r(self, x):\n",
    "        x.requires_grad_(True)\n",
    "        c = self.model(x)\n",
    "        grad_c = grad(c, x)[0]\n",
    "        c_x = grad_c[:,[0]]\n",
    "        c_xx = grad(c_x, x)[0][:,[0]]\n",
    "        \n",
    "        residual = (16/(6*9.34))*c_xx - (4/6)*c_x - 0.5*c\n",
    "        return residual\n",
    "    \n",
    "    \n",
    "    def net_0(self, x):\n",
    "        x.requires_grad_(True)\n",
    "        c = self.model(x)\n",
    "        grad_c = grad(c, x)[0]\n",
    "        c_x = grad_c[:,[0]]\n",
    "        \n",
    "        residual = (4/6) * (c-0.5) - (16/(6*9.34)) * c_x\n",
    "        return residual\n",
    "    \n",
    "    \n",
    "    def net_L(self, x):\n",
    "        x.requires_grad_(True)\n",
    "        c = self.model(x)\n",
    "        grad_c = grad(c, x)[0]\n",
    "        c_x = grad_c[:,[0]]\n",
    "        \n",
    "        residual = c_x\n",
    "        return residual        \n",
    "        \n",
    "    \n",
    "    def net_u(self, x):\n",
    "        u = self.model(x)\n",
    "        \n",
    "        return u\n",
    "    \n",
    "    \n",
    "    def loss(self, use_ad=False):\n",
    "        self.r_pred = self.net_r(self.x)\n",
    "        self.r_0_pred = self.net_0(self.x_0)\n",
    "        self.r_L_pred = self.net_L(self.x_L)\n",
    "        loss_r = torch.mean(self.r_pred**2)\n",
    "        loss_0 = torch.mean(self.r_0_pred**2)\n",
    "        loss_L = torch.mean(self.r_L_pred**2)\n",
    "\n",
    "        loss = loss_r + 100*loss_0 + 100*loss_L\n",
    "        return loss, loss_r, loss_0, loss_L\n",
    "    \n",
    "    \n",
    "    def train_Adam(self):\n",
    "        self.optimizer_Adam.zero_grad()\n",
    "\n",
    "        loss_value, loss_r_value, loss_0_value, loss_L_value = self.loss()\n",
    "        loss_value.backward()\n",
    "        self.optimizer_Adam.step()\n",
    "\n",
    "        return loss_value.item(), loss_r_value.item(), loss_0_value.item(), loss_L_value.item()\n",
    "\n",
    "        \n",
    "\n",
    "    def train_LBFGS(self):\n",
    "        loss_value, loss_r_value, loss_0_value, loss_L_value = self.loss()\n",
    "        \n",
    "        def closure():\n",
    "            loss_value, loss_r_value, loss_0_value, loss_L_value = self.loss()\n",
    "\n",
    "            self.optimizer_LBFGS.zero_grad()\n",
    "            loss_value.backward()\n",
    "\n",
    "            return loss_value\n",
    "\n",
    "        self.optimizer_LBFGS.step(closure)\n",
    "        loss_value = closure()\n",
    "        \n",
    "        return loss_value.item(), loss_r_value.item(), loss_0_value.item(), loss_L_value.item()\n",
    "    \n",
    "    \n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        loss_value, loss_r_value, loss_0_value, loss_L_value = self.loss()\n",
    "        \n",
    "        infos = 'Valid   ' + \\\n",
    "                f'Loss:{loss_value:.4e}  ' + \\\n",
    "                f'Loss_r:{loss_r_value:.4e} '\n",
    "        print(infos)\n",
    "        self.model.train()\n",
    "        return loss_value.item()\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        best_loss = 1.e10\n",
    "                 \n",
    "        for epoch in range(self.epochs_Adam):\n",
    "            loss_value, loss_r_value, loss_0_value, loss_L_value = self.train_Adam()\n",
    "            \n",
    "            if (epoch+1) % self.step_size == 0:\n",
    "                self.scheduler.step()         \n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_r_log.append(loss_r_value)\n",
    "                self.loss_0_log.append(loss_0_value)\n",
    "                self.loss_L_log.append(loss_L_value)\n",
    "                self.epoch_log.append(epoch)\n",
    "                \n",
    "                running_time = time.time() - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch+1}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_r:{loss_r_value:.2e}, loss_0:{loss_0_value:.2e}, loss_L:{loss_L_value:.2e}')  \n",
    "                \n",
    "                valid_loss = self.validate(epoch)\n",
    "                is_best = valid_loss < best_loss\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'best_loss': best_loss\n",
    "                }\n",
    "                save_model(state, is_best, save_dir=self.model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0daaeb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "trainer = Trainer_poisson(args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd76ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f'checkpoints/subnet,{args.epochs_Adam}Adam,{args.epochs_LBFGS}LBFGS'\n",
    "if os.path.exists(file)==False:\n",
    "    os.mkdir(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05626a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{file}/loss_r_log.npy',np.array(trainer.loss_r_log))\n",
    "np.save(f'{file}/loss_0_log.npy',np.array(trainer.loss_0_log))\n",
    "np.save(f'{file}/loss_L_log.npy',np.array(trainer.loss_L_log))\n",
    "np.save(f'{file}/epoch_log.npy',np.array(trainer.epoch_log))\n",
    "np.save(f'{file}/error_log.npy',np.array(trainer.error_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cc7df",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最好的模型\n",
    "state_dict = torch.load(f'{trainer.model_path}/best_model.pth')\n",
    "model.load_state_dict(state_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    \n",
    "    X_star = torch.from_numpy(X.flatten()[:,None]).float()\n",
    "\n",
    "    u_star = model(X_star)\n",
    "\n",
    "    u_star = trainer.net_u(X_star)\n",
    "    r_star = trainer.net_r(X_star)\n",
    "\n",
    "    return u_star.detach().numpy(), r_star.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 512\n",
    "x = np.linspace(0, 4, nx)\n",
    "u_star = trainset.u(x)\n",
    "u_star = u_star.reshape(-1,1)\n",
    "\n",
    "u_pred, f_u_pred = predict(x)\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "\n",
    "print('Error u: %e' % (error_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':18})\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig = plt.figure(3, figsize=(16, 5))\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    plt.plot(x, u_star, label='Exact', linewidth=3)\n",
    "    plt.plot(x, u_pred, '--', label='Predicted', linewidth=3)\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$u$')\n",
    "    plt.xticks([0, 1, 2, 3, 4])\n",
    "    plt.legend(loc='upper right', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    ax.set_aspect(1./ax.get_data_ratio())\n",
    "\n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    plt.plot(x, np.abs(u_star-u_pred), linewidth=2)\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('point-wise error')\n",
    "    plt.xticks([0, 1, 2, 3, 4])\n",
    "    plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "    plt.tight_layout()\n",
    "    ax.set_aspect(1./ax.get_data_ratio())\n",
    "\n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    plt.plot(trainer.epoch_log, trainer.loss_r_log, label='$ \\mathcal{L}_r$', linewidth=2)\n",
    "    plt.plot(trainer.epoch_log, trainer.loss_0_log, label='$ \\mathcal{L}_0$', linewidth=2)\n",
    "    plt.plot(trainer.epoch_log, trainer.loss_L_log, label='$ \\mathcal{L}_L$', linewidth=2)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc='upper right', fontsize=15)\n",
    "    ax.set_aspect(1./ax.get_data_ratio())\n",
    "    \n",
    "    plt.savefig(f'{file}/results_1D_poisson.png', dpi=96, bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef833c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
